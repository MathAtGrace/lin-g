<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-svd-uses"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Using Singular Value Decompositions </title>

  <introduction>
    <p>
      We've now seen what singular value decompositions are, how to
      construct them, and how they provide orthonormal bases for the
      four fundamental subspaces associated to a matrix.  This puts us
      in a good position to begin using singular value decompositions
      to solve a wide variety of problems.
    </p>

    <p>
      In this section, we'll revisit some familiar situations, such as
      least squares problems and principal component analysis, from
      the perspective of singular value decompositions.  Later, we
      will see how a singular value decomposition of <m>A</m> provides
      a sequence of matrices <m>A_k</m> that approximate <m>A</m>, an
      observation that will unlock some new and important applications.
    </p>

    <exploration>
      <statement>
	<p>
	  Suppose that <m>A = U\Sigma V^T</m> where
	  <me>
	    \Sigma = \begin{bmatrix}
	    13 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 8 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 2 \amp 0 \\
	    0 \amp0 \amp 0 \amp 0 \\
	    0 \amp0 \amp 0 \amp 0
	    \end{bmatrix}
	  </me>
	  and vectors <m>\uvec_j</m> form the columns of <m>U</m> and
	  <m>\uvec_j</m> form the columns of <m>V</m>.
	  <ol label="a.">
	    <li>
	      <p>
		What is the shape of the matrices <m>A</m>, <m>U</m>,
		and <m>V</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		What is the rank of <m>A</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		Describe how to find an orthonormal basis for
		<m>\col(A)</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		Describe how to find an orthonormal basis for
		<m>\nul(A)</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		If the columns of <m>Q</m> form an orthonormal basis
		for <m>\col(A)</m>, what is <m>Q^TQ</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		How would you form a matrix that projects vectors
		orthogonally onto <m>\col(A)</m>?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </exploration>

  </introduction>

  <subsection>
    <title> Rank <m>k</m> approximations </title>

    <p>
      If we have a singular value decomposition for a matrix <m>A</m>,
      we can form a series of matrices <m>A_k</m> that approximate
      <m>A</m>.  Readers who are familiar with calculus will recognize
      a similarity to the sort of approximations seen there:  a
      function <m>f(x)</m> can be approximated by a linear function, a
      quadratic functions, and so forth.
    </p>

    <p>
      We will first explain the approximations and then explore how
      they can be used.  First, imagine that we have a singular value
      decomposition of the
      <m>m\times n</m> matrix <m>A=U\Sigma V^T</m> and that the rank
      of <m>A</m> is <m>r</m>.  If we write an <m>n</m>-dimensional
      vector <m>\xvec</m> as a linear combination of right singular
      vectors, we have
      <me>
	\xvec=c_1\vvec_1 + c_2\vvec_2 + \ldots + c_n\vvec_n.
      </me>
      Notice that <m>c_j = \vvec_n\cdot\xvec = \vvec_n^T\xvec</m>.
      We then have
      <md>
	<mrow>
	  A \xvec \amp = A(c_1\vvec_1 + c_2\vvec_2 + \ldots +
	  c_n\vvec_n)
	</mrow>
	<mrow>
	  \amp = c_1A\vvec_1 + c_2A\vvec_2 + \ldots + c_nA\vvec_n
	</mrow>
	<mrow>
	  \amp = c_1\sigma_1\uvec_1 + c_2\sigma_2\uvec_2 + \ldots
	  c_r\sigma_r\uvec_r
	</mrow>
	<mrow>
	  \amp = \sigma_1\uvec_1\vvec_1^T\xvec +
	  \sigma_2\uvec_2\vvec_2^T\xvec + \ldots +
	  \sigma_r\uvec_r\vvec_r^T\xvec.
	</mrow>
      </md>
      We therefore have
      <me>
	A = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + \ldots +
	  \sigma_r\uvec_r\vvec_r^T.
      </me>
      In other words, we can write <m>A</m> as a sum of matrices each
      of which has the form <m>\sigma_j\uvec_j\vvec_j^T</m>. 
    </p>

    <p>
      Remember that we have ordered the singular values so that
      <m>\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r</m>.  This
      means that the first singular value is the largest one so we can
      expect that the first term in this sum,
      <m>\sigma_1\uvec_1\vvec_1^T</m> is the most important, and
      second term <m>\sigma_2\uvec_2\vvec_2^T</m> the next most
      important term and so forth.  This
      leads us to define the series of approximating matrices
      <md>
	<mrow>
	  A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T
	</mrow>
	<mrow>
	  A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T
	</mrow>
	<mrow>
	  A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + 
	  \sigma_3\uvec_3\vvec_3^T
	</mrow>
	<mrow>
	  \vdots \amp
	</mrow>
	<mrow>
	  A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + \ldots +
	  \sigma_r\uvec_r\vvec_r^T
	</mrow>
      </md>
    </p>

    <p>
      There is a simple way to form these approximations
      computationally from a singular value decomposition <m>A=U\Sigma
      V^T</m>.  Notice that the approximation <m>A_k</m> has the same
      singular vectors as <m>A</m> and same first <m>k</m> singular
      values.  Therefore, we can form <m>A_k</m> by modifying
      <m>\Sigma</m> so that the singular values
      beyond the first <m>k</m> are zero.  For instance, if
      <m>
	\Sigma = \begin{bmatrix}
	22 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 14 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 3 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	\end{bmatrix}
	</m>, we can form <m>A_2</m> by replacing <m>\Sigma</m> with
	<m>
	\begin{bmatrix}
	22 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 14 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	\end{bmatrix}
      </m>.
    </p>

    <activity>
      <p>
	Let's consider a matrix <m>A=U\Sigma V^T</m> where
	<md>
	  <mrow>
	    \amp U = \begin{bmatrix}
	    \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    \frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
	    \end{bmatrix},\hspace{10pt}
	    \Sigma = \begin{bmatrix}
	    500 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 100 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 20 \amp 0  \\
	    0 \amp 0 \amp 0 \amp 4
	    \end{bmatrix}
	  </mrow>
	  <mrow>
	    \amp V = \begin{bmatrix}
	    \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
	    -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2}
	    \end{bmatrix}
	  </mrow>
	</md>
	Evaluating the following cell will create the matrices
	<c>U</c>, <c>V</c>, and <c>Sigma</c>.  Notice how the
	<c>diagonal_matrix</c> command provides a convenient way to
	form a diagonal 
	matrix. 
	<sage>
	  <input>
h = 1/2
U = matrix(4,4,[h,h,h,h,  h,h,-h,-h,  h,-h,h,-h,  h,-h,-h,h])	    
V = matrix(4,4,[h,h,h,h,  h,-h,-h,h,  -h,-h,h,h,  -h,h,-h,h])
Sigma = diagonal_matrix([500, 100, 20, 4])
	  </input>
	</sage>
	<ol label="a.">
	  <li>
	    <p>
	      Form the matrix <m>A=U\Sigma V^T</m>.  What is
	      <m>\rank(A)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Now form the approximating matrix <m>A_1=U\Sigma_1
	      V^T</m>.  What is <m>\rank(A_1)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Find the error in the approximation, which may be
	      expressed as
	      <m>A-A_1</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Now find <m>A_2 = U\Sigma_2 V^T</m> and the error
	      <m>A-A_2</m>.  What is <m>\rank(A_2)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Find <m>A_3 = U\Sigma_3 V^T</m> and the error
	      <m>A-A_3</m>.  What is <m>\rank(A_3)</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      What would happen if we were to compute <m>A_4</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      Suppose that another matrix has the singular value
	      decomposition <m>U\Sigma V^T</m> where
	      <me>
		U = \begin{bmatrix}
		\uvec_1 \amp \uvec_2 \amp \uvec_3
		\end{bmatrix},\hspace{10pt}
		\Sigma = \begin{bmatrix}
		5 \amp 0 \\
		0 \amp 3 \\
		0 \amp 0
		\end{bmatrix}.
	      </me>
	      Suppose also that we define new matrices by trimming off
	      the last column of <m>U</m> and the last row of
	      <m>\Sigma</m>: 
	      <me>U_2 = \begin{bmatrix}
		\uvec_1 \amp \uvec_2 
		\end{bmatrix},\hspace{10pt}
		\Sigma_2 = \begin{bmatrix}
		5 \amp 0 \\
		0 \amp 3 \\
		\end{bmatrix}.
	      </me>
	      Explain why <m>U\Sigma = U_2\Sigma_2</m>.
	    </p>
	  </li>
	</ol>
	
      </p>
    </activity>

    <p>
      In this activity, we saw that the approximation <m>A_k</m> has
      rank <m>k</m> because its singular value decomposition has
      <m>k</m> nonzero singular values.  We also saw how the
      difference between <m>A</m> and the approximations <m>A_k</m>
      decrease as <m>k</m> increases, which means that <m>A_k</m>
      forms a better approximation as <m>k</m> increases.  In fact,
      <m>A_k</m> is the matrix that is closest to <m>A</m>, in a sense
      that can be made precise, among all rank <m>k</m> matrices.
    </p>

    <p>
      The last part of this activity indicates how we can represent
      <m>A</m> and its approximations <m>A_k</m> more efficiently.  If
      we remember that the columns of the matrix product
      <m>U\Sigma</m> are formed by multiplying the columns of
      <m>\Sigma</m> by <m>U</m>, then we have
      <me>
	U\Sigma =
	\begin{bmatrix}
	\uvec_1\amp\uvec_2\amp\uvec_3
	\end{bmatrix}
	\begin{bmatrix}
	5 \amp 0 \\
	0 \amp 3 \\
	0 \amp 0
	\end{bmatrix}
	= \begin{bmatrix}
	5\uvec_1 \amp 3\uvec_2
	\end{bmatrix}
	= \begin{bmatrix}
	\uvec_1\amp\uvec_2
	\end{bmatrix}
	\begin{bmatrix}
	5 \amp 0 \\
	0 \amp 3 \\
	\end{bmatrix}
	= U_2\Sigma_2.
      </me>
    </p>

    <p>
      More generally, if we trim columns from <m>U</m> and <m>V</m>
      and form the square matrix <m>\Sigma_k</m>, such as
      <me>
	U_k = \begin{bmatrix}
	\uvec_1 \amp \uvec_2 \amp \ldots \uvec_k
	\end{bmatrix},\hspace{10pt}
	\Sigma_k = \begin{bmatrix}
	\sigma_1 \amp 0 \amp \ldots \amp 0 \\
	0 \amp \sigma_2 \amp \ldots \amp 0 \\
	\vdots \amp \vdots \amp \ddots \amp \vdots \\
	0 \amp 0 \amp \ldots \amp \sigma_k
	\end{bmatrix},\hspace{10pt}
	V_k = \begin{bmatrix}
	\vvec_1 \amp \vvec_2 \amp \ldots \vvec_k
	\end{bmatrix},
      </me>
      then we have <m>A_k = U_k\Sigma_kV_k^T</m>.  Notice that, if the
      rank of <m>A</m> is <m>r</m>, then we have
      <m>A=U_r\Sigma_rV_r</m>.  We called this the <em>reduced</em>
      singular value decomposition of <m>A</m>.
    </p>

  </subsection>
      
  <subsection>
    <title> Least squares problems </title>

    <p>
      Least squares problems, which we explored in <xref
      ref="sec-least-squares" />, arise when we are confronted with an
      inconsistent linear system <m>A\xvec=\bvec</m>.  Since there is
      no solution to the system, we instead find the vector
      <m>\xvec</m> that minimizes the distance between <m>\bvec</m>
      and <m>A\xvec</m>. 
      That is, we find the
      vector <m>\xhat</m>, the least squares approximate solution, by
      solving <m>A\xhat=\bhat</m> where <m>\bhat</m> is the orthogonal
      projection of <m>\bvec</m> onto the column space of <m>A</m>.
    </p>

    <p>
      If we have a decomposition <m>A=U\Sigma V^T</m>, then the number
      of nonzero singular values <m>r</m> tells us the rank of
      <m>A</m>, and the first <m>r</m> columns of <m>U</m> form an
      orthonormal basis for <m>\col(A)</m>.  This basis may be used to
      project vectors onto <m>\col(A)</m> and hence to solve least
      squares problems.
    </p>

    <p>
      Before exploring this connection further, we will introduce Sage 
      as a tool for automating the construction of singular value
      decompositions.  One new feature is that we need to declare our
      matrix to consist of floating point entries.  We do this by
      including <c>RDF</c> inside the matrix definition, as
      illustrated in the following cell.
      <sage>
	<input>
A = matrix(RDF, 3, 2, [1,0,-1,1,1,1])
U, Sigma, V = A.SVD()	  
print(U)
print(Sigma)
print(V)
	</input>
      </sage>
    </p>

    <activity>
      <statement>
	<p>
	  Consider the equation <m>A\xvec=\bvec</m> where
	  <me>
	    \begin{bmatrix}
	    1 \amp 0 \\
	    1 \amp 1 \\
	    1 \amp 2
	    \end{bmatrix}
	    \xvec = \threevec{-1}36
	  </me>

	  <ol label="a.">
	    <li>
	      <p>
		Find a singular value decomposition for <m>A</m> using
		the Sage cell below.  What are singular values of
		<m>A</m>?
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	    <li>
	      <p>
		What is <m>r</m>, the rank of <m>A</m>?  How can we
		identify an orthonormal basis for <m>\col(A)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose we write <m>\bvec</m> as a linear combination
		of the columns of <m>U</m>:
		<me>
		  \bvec = c_1\uvec_1 + c_2\uvec_2 + c_3\uvec_3.
		</me>
		<ol label="i.">
		  <li>
		    <p>
		      What condition on the weights <m>c_1</m>, <m>c_2</m>,
		      and <m>c_3</m> determine whether <m>A\xvec = \bvec</m>
		      is consistent?
		    </p>
		  </li>
		  <li>
		    <p>
		      Similarly, what condition on <m>U^T\bvec</m>
		      determines whether <m>A\xvec=\bvec</m> is
		      consistent.
		    </p>
		  </li>
		  <li>
		    <p>
		      Evaluate <m>U^T\bvec</m> and determine whether
		      <m>A\xvec=\bvec</m> is consistent.
		      <sage>
			<input>
			</input>
		      </sage>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Form the reduced singular value decomposition
		<m>U_r\Sigma_rV_r^T</m> by constructing the matrix
		<m>U_r</m>, consisting of 
		the first <m>r</m> columns of <m>U</m>, <m>V_r</m>, 
		consisting of the first <m>r</m> columns of <m>V</m>,
		and <m>\Sigma_r</m>, an <m>r\times r</m> matrix.
		Verify that <m>A=U_r\Sigma_r V_r^T</m>.
	      </p>
	      <p>
		You may find it convenient to remember that, if
		<c>B</c> is a matrix defined in Sage, then
		<c>B.matrix_from_columns( list )</c> and
		<c>B.matrix_from_rows( list )</c> can be used to
		extract columns or rows from <c>B</c>.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		How does the reduced singular value decomposition
		provide a matrix whose columns are an orthonormal basis
		for <m>\col(A)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Explain why <m>\xhat</m>, the least squares
		approximate solution, satisfies
		<me>
		  A\xhat = U_r\Sigma_rV_r^T\xhat = U_rU_r^T\bvec.
		</me>
	      </p>
	    </li>

	    <li>
	      <p>
		What is the product <m>U_r^TU_r</m>?  What do you find
		if you multiply both sides of <m>U_r\Sigma_rV_r^T\xhat
		= U_rU_r^T\bvec</m> by <m>U_r^T</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Explain why
		<me>
		  \xhat = V_r\Sigma_r^{-1}U_r^T\bvec
		</me>
		and use this expression to find <m>\xhat</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	      
	  </ol>
	</p>
      </statement>
    </activity>
		
    <p>
      Using the singular value decomposition <m>A=U\Sigma V^T</m>
      provided by Sage in the activity, 
      we see that <m>\rank(A)=2</m> because there are
      two nonzero singular values.  This means that the column space
      is two-dimensional and that <m>\uvec_1</m> and
      <m>\uvec_2</m>, the first two columns of <m>U</m>, form an
      orthonormal basis for <m>\col(A)</m>.
    </p>

    <p>
      If we are given a three-dimensional vector <m>\bvec</m>, we may
      write it as a linear cominbation of the columns of <m>U</m>;
      that is, <m>\bvec=c_1\uvec_1+c_2\uvec_2+c_3\uvec_3</m>.  Since
      <m>\uvec_1</m> and <m>\uvec_2</m> form a basis for <m>
      \col(A)</m>, we see that <m>A\xvec=\bvec</m> is only consistent
      if <m>c_3 = 0</m>.  Notice that
      <me>
	U^T\bvec = \threevec{\uvec_1\cdot\bvec}
	{\uvec_2\cdot\bvec}
	{\uvec_3\cdot\bvec}
	=\threevec{c_1}{c_2}{c_3},
      </me>
      which implies that <m>A\xvec = \bvec</m> is consistent if and
      only if the
      third component of <m>U^T\bvec</m> is zero.
    </p>

    <p>
      Since the rank <m>r</m> of <m>A</m> is 2, the reduced singular
      value decomposition is <m>A=U_2\Sigma_2V_2^T</m> where
      <me>
	U_2=\begin{bmatrix} \uvec_1\amp\uvec_2 \end{bmatrix},
	\hspace{10pt}
	\Sigma_2 = \begin{bmatrix}
	\sigma_1 \amp 0 \\
	0 \amp \sigma_2
	\end{bmatrix},\hspace{10pt}
	V_2 = \begin{bmatrix}
	\vvec_1\amp\vvec_2
	\end{bmatrix} = V.
      </me>
      Notice that the columns of <m>U_2</m> form an orthonormal basis
      for <m>\col(A)</m>.  Therefore, the vector <m>\bhat</m> that
      results from projecting <m>\bvec</m> onto <m>\col(A)</m> is
      <m>\bhat = U_2U_2^T\bvec</m>.  In other words, we have
      <me>
	<mrow>
	  A\xhat \amp = \bhat
	</mrow>
	<mrow>
	  U_2\Sigma_2V_2^T \xhat \amp = U_2U_2^T\bvec.
	</mrow>
      </me>
    </p>

    <p>
      Remembering that <m>U_2^TU_2=I_2</m>, the <m>2\times2</m>
      identity, and multiplying by <m>U_2^T</m>, we have
      <md>
	<mrow>
	  U_2^T ~U_2\Sigma_2V_2^T\xhat \amp = U_2^T~U_2U_2^T\bvec
	</mrow>
	<mrow>
	  \Sigma_2V_2^T\xhat \amp = U_2^T\bvec
	</mrow>
	<mrow>
	  V_2^T\xhat\amp = \Sigma_2^{-1} U_2^T\bvec
	</mrow>
	<mrow>
	  \xhat\amp = V\Sigma_2^{-1} U_2^T\bvec.
	</mrow>
      </md>
      Here, we have used the fact that <m>V_2=V</m>, an
      orthogonal matrix, which follows from the fact that the columns
      of <m>A</m> are linearly independent so that <m>\nul(A)=0</m>.
    </p>

    <proposition xml:id="prop-svd-ols">
      <statement>
	<p>
	  If the columns of <m>A</m> are linearly independent and the
	  reduced singular value decomposition of <m>A</m> is
	  <m>A=U_r\Sigma_rV_r^T</m>, then <m>\xhat</m>, the least
	  squares approximate solution to <m>A\xvec=\bvec</m> is given
	  by
	  <me>
	    \xhat = V_r\Sigma_r^{-1}U_r^T\bvec.
	  </me>
	</p>
      </statement>
    </proposition>

    <!--
	Homework:  moore-penrose inverse
	B B.dagger = I
    -->
      
      
  </subsection>

  <subsection>
    <title> Principal component analysis </title>

    <p>
      In <xref ref="sec-pca" />, we explored principal component
      analysis as a technique to reduce the dimension of a data set.
      In particular, we constructed the covariance matrix <m>C</m>
      from a demeaned data matrix and saw that the eigenvectors of
      <m>C</m> gave us a way to understand the variance of the data
      set.  We called these eigenvectors <em>principal components</em>
      and found that projecting the data onto the principal components
      having the largest associated eigenvalues frequently gave us a
      way to visualize the data set.
    </p>

    <p>
      The approximations <m>A_k</m> formed from a singular
      value decomposition of <m>A</m> give us another means of
      approaching principal component analysis, as the next activity
      illustrates.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose that we have a data set with <m>N</m> points and
	  that <m>A</m> represents the demeaned data matrix.
	  <ol label="a.">
	    <li>
	      <p>
		Given a matrix <m>B</m>, of what matrix do the right
		singular vectors of <m>B</m> appear as eigenvectors?
	      </p>
	    </li>

	    <li>
	      <p>
		If we have <m>B=U\Sigma V^T</m>, remember that
		<m>B^T=V\Sigma^T U^T</m>.  In other words,
		the left singular vectors of <m>B</m>
		are the right singular vectors of <m>B^T</m>.  Of what
		matrix do the left singular vectors of <m>B</m>
		appear as eigenvectors?
	      </p>
	    </li>

	    <li>
	      <p>
		Consider now our demeaned data matrix and a singular
		value decomposition <m>A=U\Sigma V^T</m>.  Of what
		matrix do the left singular vectors, the columns of
		<m>U</m>, appear as eigenvectors?  Explain how this
		matrix is related to the covariance matrix <m>C</m>.
		Then explain why the columns of <m>U</m> are the
		principal components of <m>A</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		How are the singular values of <m>A</m> related to the
		eigenvalues of the covariance matrix <m>C</m>.  In
		particular, how is the variance <m>V_{\uvec_j}</m>
		expressed in terms of the singular values of <m>A</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Let's revisit the iris data set that we studied in
		<xref ref="sec-pca" />.  Remember that there are four
		measurements given for each of 150 irises.  The irises
		are grouped into three species of which there are 50
		each.
	      </p>

	      <p>
		Evaluating the following cell will load the demeaned
		data matrix <m>A</m> whose shape is <m>4\times150</m>.
		<sage>
		  <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/pca-iris.py', globals())
df.head()
		  </input>
		</sage>
	      </p>

	      <p>
		Find the singular values of <m>A</m> using the command
		<c>A.singular_values()</c> and use them to determine
		the fraction of variance explained by the first two
		principal components.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		We will now write the matrix <m>\Gamma = \Sigma
		V^T</m> so that <m>A = U\Gamma</m>.
	      </p>

	      <p>
		Suppose that a demeaned data point, say, the 100th
		column of <m>A</m>, is written as a
		linear combination of principal components:
		<me>
		  \xvec = c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4.
		</me>
		Explain why the vector
		<m>\fourvec{c_1}{c_2}{c_3}{c_4}</m> appears as 100th
		column of <m>\Gamma</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Let's now consider the approximation
		<m>A_2=U_2\Sigma_2V_2^T</m> of the demeaned data
		matrix <m>A</m>, which involves the first two
		principal components.  Explain why the 100th column of
		<m>A_2</m> represents the projection of <m>\xvec</m>
		onto the two-dimensional subspace spanned by the first
		two principal components, <m>\uvec_1</m> and
		<m>\uvec_2</m>.  Then explain why the coefficients in
		that projection, <m>c_1\uvec_1 + c_2\uvec_2</m>, form
		a two-dimensional vector <m>\twovec{c_1}{c_2}</m> that
		is the 100th column of <m>\Gamma_2=\Sigma_2
		V_2^T</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Now we've seen that the columns of <m>\Gamma_2 =
		\Sigma_2 V_2^T</m> form the coordinates of the
		demeaned data points projected on the two-dimensional
		subspace spanned by <m>\uvec_1</m> and <m>\uvec_2</m>.
		In the cell below, find a singular value decomposition
		of <m>A</m> and use it to form the matrix
		<c>Gamma2</c> below.  When you 
		evaluate this cell, you will see a plot of the
		projected demeaned data plots, similar to the one we
		created in <xref ref="sec-pca" />.
		<sage>
		  <input>
# Form the SVD of A and use it to form Gamma2

		    
Gamma2 = 

# The following will plot the projected demeaned data points
data = Gamma2.columns()
(list_plot(data[:50], color='blue', aspect_ratio=1) +
 list_plot(data[50:100], color='orange') +
 list_plot(data[100:], color='green'))
		  </input>
		</sage>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

    <p>
      This activity demonstrates the connection between singular value
      decompositions and principal component analysis.  In particular,
      we can see that the principal components of a data set are the
      left singular vectors of the demeaned data matrix <m>A</m>.
    </p>

    <p>
      To begin, remember that the left singular vectors of a matrix
      <m>A</m> are the right singular vectors of <m>A^T</m> and that
      the right singular vectors of <m>A^T</m> are the eigenvectors of
      its Gram matrix <m>(A^T)^TA^T = AA^T</m>.  Now if there are
      <m>N</m> data points, then the covariance
      matrix has the form <m>C=\frac 1N ~AA^T</m>.  This says that the
      left singular vectors of <m>A</m> are the eigenvectors of the
      covariance matrix <m>C</m>, and we called those eigenvectors the
      principal components.
    </p>

    <p>
      Moreover, if <m>\lambda_j</m> is an eigenvalue of the covariance
      matrix <m>C=\frac1N~AA^T</m>, then <m>N\lambda_j</m> is an
      eigenvalue of <m>AA^T</m>, 
      which is <m>\sigma_j^2</m>, the square of a singular value of
      <m>A</m>.  Therefore, we find that the variance in the direction of a
      principal component is
      <me>
	V_{\uvec_j} = \lambda_j = \frac{\sigma_j^2}{N}.
      </me>
      This shows that the variance is easily expressed in terms of the
      singular values of the demeaned data matrix.  When we apply this
      to the iris data set, we see that 97.8% of the variance in the
      data is explained by the first two components.  This agrees with
      the result we found in <xref ref="sec-pca"/>.
    </p>

    <p>
      In addition, we are interested in representing the data points
      as projected onto the first two principal components.  If a data
      point <m>\xvec</m> is represented as a linear combination of
      principal components
      <me>
	\xvec=c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4,
      </me>
      then the projection formula tells us that the projection of
      <m>\xvec</m> onto the first two principal components is
      <m>c_1\uvec_1 + c_2\uvec_2</m>.  The data point <m>\xvec</m>
      appears as a column in the data matrix <m>A</m> and its
      projection as the corresponding column in <m>A_2</m>.  Since we
      have
      <me>
	A_2=U_2\Sigma_2V_2^T = U_2\Gamma_2 =
	\begin{bmatrix}
	\uvec_1 \amp \uvec_2
	\end{bmatrix} \Gamma_2,
      </me>
      then the
      coordinate vector <m>\twovec{c_1}{c_2}</m> appears as the
      corresponding column of <m>\Gamma_2</m>.  To construct a plot of
      the points projected onto the first two principal components, we
      see that we only need to plot the points represented by the
      columns of <m>\Gamma_2</m>.
    </p>

    <p>
      To summarize, we see that singular value decompositions give us
      an alternative way to visualize data sets using principal
      component analysis.  The variance in the direction of the
      principal components is related to the singular values through a
      simple expression, and the coordinates of the projected demeaned
      data points appear as the columns of <m>\Gamma_k =
      \Sigma_kV_k^T</m>.
    </p>

  </subsection>

  <subsection>
    <title> Image compressing and denoising </title>

    <p>
      We have seen that the singular value decomposition of a data
      matrix <m>A</m> can be described in terms of the principal
      components of <m>A</m>.  We will now put these approximations to
      use in a new way by exploring their application to some issues
      in image processing.
    </p>

    <p>
      In <xref ref="sec-jpeg" />, we studied the JPEG compression
      algorithm, whose foundation is the change of basis defined by
      the Discrete Cosine Transform.  We'll see here how the singular
      value decomposition provides another tool for both compressing
      images and removing noise in them.
    </p>

    <activity>
      <statement>
	<p>
	  Evaluating the following cell loads some data that we'll use
	  in this activity.  To begin, it defines and displays a
	  <m>25\times15</m> matrix <m>A</m>.
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/svd-compress.py', globals())
print(A)


	    </input>
	  </sage>
	  <ol label="a.">
	    <li>
	      <p> 
		If we interpret 0 as black and 1 as white, this matrix
		represents an image as shown below.
		<sage>
		  <input>
display_matrix(A)

		  </input>
		</sage>
		We will explore how the singular value
		decomposition helps us to compress this image.
		<ol label="i.">
		  <li>
		    <p>
		      By inspecting the image represented by <m>A</m>,
		      identify a basis for <m>\col(A)</m> and
		      determine <m>\rank(A)</m>.
		    </p>
		  </li>

		  <li>
		    <p>
		      The following cell plots the singular values of
		      <m>A</m>.  Explain how this plot verifies that
		      the rank is what you found in the previous part.
		      <sage>
			<input>
plot_sv(A)		    

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      There is a command <c>approximate(A, k)</c> that
		      creates the approximation <m>A_k</m>.  Use the
		      cell below to define <m>k</m> and look at the
		      images represented by the
		      first few approximations.  What is the smallest
		      value of <m>k</m> for which <m>A=A_k</m>?
		      <sage>
			<input>
k = 
display_matrix(approximate(A, k))			  
			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Now we can see how the singular value
		      decomposition allows us to compress images.
		      Since this is a <m>25\times15</m> matrix, we need
		      <m>25\cdot15=375</m> numbers to
		      represent the image.  However, we can also
		      reconstruct the image using a small number of
		      singular values and vectors:
		      <me>
			A = A_k = \sigma_1\uvec_1\vvec_1^T +
			\sigma_2\uvec_2\vvec_2^T + \ldots +
			\sigma_k\uvec_k\vvec_k^T.
		      </me>
		      What are the dimensions of the singular vectors
		      <m>\uvec_i</m> and <m>\vvec_i</m>?  Between the
		      singular vectors and singular values, how many
		      numbers do we need to reconstruct
		      <m>A_k</m> for the smallest <m>k</m> for which
		      <m>A=A_k</m>?
		    </p>
		  </li>

		  <li>
		    <p>
		      What fraction of the total 375 numbers does this
		      represent?  The <em>compression ratio</em> is
		      the ratio of the uncompressed size to the
		      compressed size.  What compression ratio does
		      this represent?
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Next we'll explore an example based on a photograph. 
		<ol label="i">
		  <li>
		    <p>
		      Consider the following image consisting of an
		      array of <m>316\times310</m> pixels stored in the
		      matrix <m>A</m>.
		      <sage>
			<input>
A = matrix(RDF, image)
display_image(A)

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Plot the singular values of <m>A</m>.
		      <sage>
			<input>
plot_sv(A)
			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Use the cell below to study the approximations
		      <m>A_k</m> for <m>k=1, 10, 20, 50, 100</m>.
		      <sage>
			<input>
k = 1
display_image(approximate(A, k))		    

			</input>
		      </sage>
		      What is the compression ratio when <m>k=50</m>?
		      What is the compression ratio when <m>k=100</m>?
		      Notice how a higher compression ratio leads to a
		      lower quality reconstruction of the image.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		A second, related application of the singular value
		decomposition to image processing is called
		<em>denoising</em>.  For example, consider the image
		represented by the matrix <m>A</m> below.
		<sage>
		  <input>
A = matrix(RDF, noise.values)		    
display_matrix(A)		
		  </input>
		</sage>
		This image is similar to the image of the letter "O"
		we first studied in this activity, but there are
		splotchy regions in the background that result,
		perhaps, from scanning the image.  We think of the
		splotchy regions as noise, and our goal is to improve
		the quality of the image by reducing the noise.

		<ol label="i.">
		  <li>
		    <p>
		      Plot the singular values below.  How are the
		      singular values of this matrix similar to those
		      represented by the clean image that we 
		      considered earlier and how are they different?
		      <sage>
			<input>
plot_sv(A)

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      There is a natural point where the singular
		      values dramatically decrease so it makes sense
		      to think of the noise as being formed by the
		      small singular values.  To denoise the image, we
		      will therefore replace <m>A</m> by its
		      approximation <m>A_k</m>, where <m>k</m> is the
		      point at which the singular values drop off.
		      Replacing <m>A\approx A_k</m> has the effect of
		      setting the small singular values to zero.
		      Choose an appropriate value of <m>k</m> below
		      and notice how the new image is somewhat cleaned
		      up as a result of removing the noise.
		      <sage>
			<input>
k = 
display_matrix(approximate(A, k))

			</input>
		      </sage>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

  </subsection>

  <subsection>
    <title>
      Analyzing Supreme Court cases
    </title>

    <p>
      As we've seen, the singular value decomposition concentrates the
      most important features of a matrix into the first singular
      values and singular vectors.  We will now use this observation
      to extract meaning from a large data set giving the
      voting records of Supreme Court justices.  A similar analysis
      appears in the paper <url
      href="https://www.pnas.org/content/100/13/7432"> A pattern
      analysis of the second Rehnquist U.S. Supreme Court </url> by
      Lawrence Sirovich.  
    </p>

    <p>
      The makeup of the Supreme Court was unusually stable during a
      period from 1994-2005 when it was led by Chief Justice William
      Rehnquist.  This is sometimes called the <em>second Rehnquist
      court</em>.  The justices during this period were:
      <ul>
	<li><p> William Rehnquist </p></li>
	<li><p> Antonin Scalia </p></li>
	<li><p> Clarence Thomas </p></li>
	<li><p> Anthony Kennedy </p></li>
	<li><p> Sandra Day O'Connor </p></li>
	<li><p> John Paul Stevens </p></li>
	<li><p> David Souter </p></li>
	<li><p> Ruth Bader Ginsburg </p></li>
	<li><p> Stephen Breyer </p></li>
      </ul>
    </p>

    <p>
      During this time, there were 911 cases in which all nine
      judges voted.  We'd like to understand patterns in these votes.
    </p>

    <activity>
      <statement>
	<p>
	  Evaluating the following cell loads in some data and
	  displays a particular data set describing the votes of each
	  justice in these 911 cases.  An entry of +1 means that
	  justice voted with the majority while -1 means that justice
	  was in the minority.  This information is also stored in the
	  <m>9\times911</m> matrix <m>A</m>.
	  
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/svd-supreme.py', globals())
A = matrix(RDF, cases.values)
cases

	    </input>
	  </sage>
	  The justices are listed, very roughly, in order from more
	  conservative to more progressive.
	</p>

	<p>
	  In this activity, it will be helpful to visualize the
	  entries in various matrices and vectors.  The next cell
	  displays the matrix <m>A</m> with white
	  representing an entry of +1, red representing -1, and black
	  representing 0. 
	  <sage>
	    <input>
display_matrix(A.matrix_from_columns(range(50)))


	    </input>
	  </sage>
	  <ol label="a.">
	    <li>
	      <p>
		Plot the singular values of <m>A</m> below.  Describe
		the significance of this plot, including the relative
		contributions from the singular values <m>\sigma_k</m>
		as <m>k</m> increases.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Form the singular value decomposition <m>A=U\Sigma
		V^T</m> and the matrix of coefficients <m>\Gamma</m>
		so that <m>A=U\Gamma</m>.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		We will now study a particular case, the second case
		appearing as the column of <m>A</m> indexed
		by <c>1</c>.
		There is a command <c>display_column(A, k)</c> that
		provides a visual display of the <m>k^{th}</m> column
		of a matrix <m>A</m>.  Describe the justices' votes in
		the second case.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Also, display the first left singular vector
		<m>\uvec_1</m>, the column of 
		<m>U</m> indexed by <m>0</m>, and the column of
		<m>\Gamma</m> holding the coefficients that express the
		second case as a linear combination of left singular
		vectors.
		<sage>
		  <input>

		  </input>
		</sage>

		What does this tell us about how the second case is
		constructed as a linear combination of left singular
		vectors?  What is the significance of the first left
		singular vector?
	      </p>
	    </li>

	    <li>
	      <p>
		Let's now study the <m>48^{th}</m> case, which is
		represented by the column of <m>A</m> indexed by
		<c>47</c>.  Describe the voting pattern in this case.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Display the second left singular vector <m>\uvec_2</m>
		and the vector 
		of coefficients that express the <m>48^{th}</m> case as
		a linear combination of left singular vectors.
		<sage>
		  <input>

		  </input>
		</sage>
		Describe how this case is constructed as a linear
		combination of singular vectors.  What is the
		significance of the second left singular vector?
	      </p>
	    </li>

	    <li>
	      <p>
		The data in <xref ref="table-supreme-cases" /> describes
		the number of cases decided by a particular count of
		votes. 
		<table xml:id="table-supreme-cases">
		  <title> Number of cases by vote count </title>
		  <tabular halign="center">
		    <row bottom="minor">
		      <cell> Vote count </cell>
		      <cell> # of cases </cell>
		    </row>
		    <row>
		      <cell> 9-0 </cell>
		      <cell> 405 </cell>
		    </row>
		    <row>
		      <cell> 8-1 </cell>
		      <cell> 89 </cell>
		    </row>
		    <row>
		      <cell> 7-2 </cell>
		      <cell> 111 </cell>
		    </row>
		    <row>
		      <cell> 6-3 </cell>
		      <cell> 118 </cell>
		    </row>
		    <row>
		      <cell> 5-4 </cell>
		      <cell> 188 </cell>
		    </row>
		  </tabular>
		</table>
		How do the singular vectors <m>\uvec_1</m> and
		<m>\uvec_2</m> reflect this data?  Would you
		characterize the court as leaning toward the
		conservatives or progressives?  Use these singular
		vectors to explain your response.
	      </p>
	    </li>

	    <li>
	      <p>
		Cases decided by a 5-4 vote are often the most
		impactful as they represent a sharp divide among the
		justices and, often, society at large.  For that
		reason, we will now focus on the 5-4 decisions.
		Evaluating the next cell forms the <m>9\times188</m>
		matrix <m>A</m> consisting of 5-4 decisions.
		<sage>
		  <input>
A = matrix(RDF, fivefour.values)
display_matrix(A.matrix_from_columns(range(50)))

		  </input>
		</sage>
		Form the singular value decomposition of <m>A=U\Sigma
		V^T</m> 
		along with the matrix <m>\Gamma</m> of coefficients so
		that <m>A=U\Gamma</m> and display the first left
		singular vector <m>\uvec_1</m>.  Study how the
		<m>7^{th}</m> case is constructed as a linear
		combination of left singular vectors.
		<sage>
		  <input>

		  </input>
		</sage>
		What does this singular vector tell us about the
		make up of the court and whether it leans towards the
		conservatives or progressives?  
	      </p>
	    </li>

	    <li>
	      <p>
		Display the second left singular vector
		<m>\uvec_2</m> and study how the <m>6^{th}</m> case,
		indexed by <c>5</c>, is constructed as a linear
		combination of left singular vector.
		<sage>
		  <input>

		  </input>
		</sage>
		What does <m>\uvec_2</m> tell us about the
		relative importance of the justices' voting records?
	      </p>
	    </li>

	    <li>
	      <p>
		By a <em>swing vote</em>, we mean a justice who is
		less inclined to vote with a particular block of
		justices but instead swings from one block to
		another with the potential to sway close decisions.
		What do the singular vectors <m>\uvec_1</m> and
		<m>\uvec_2</m> tell us about the presence of voting
		blocks on the court and and presence of a swing vote?
		Which justice represents the swing vote?
	      </p>
	    </li>
		
	  </ol>
	</p>

	
      </statement>
    </activity>
  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section has demonstrated some uses of the singular value
      decomposition.  Because the singular values appear in decreasing
      order, the decomposition has the effect of concentrating
      the most important features of the matrix into the first
      singular values and singular vectors.
      <ul>
	<li>
	  <p>
	    The singular value decomposition of <m>A</m> leads to a
	    series of approximations <m>A_k</m> of <m>A</m> where 
	    <md>
	      <mrow>
		A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T
	      </mrow>
	      <mrow>
		A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T
	      </mrow>
	      <mrow>
		A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T + 
		\sigma_3\uvec_3\vvec_3^T
	      </mrow>
	      <mrow>
		\vdots \amp
	      </mrow>
	      <mrow>
		A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T + \ldots +
		\sigma_r\uvec_r\vvec_r^T
	      </mrow>
	    </md>
	    In each case, <m>A_k</m> is the rank <m>k</m> matrix that
	    is closest, in a specific sense, to <m>A</m>.
	  </p>
	</li>

	<li>
	  <p>
	    Because the first left singular vectors form an
	    orthonormal basis for <m>\col(A)</m>, a singular value
	    decomposition provides a convenient way to project vectors
	    onto <m>\col(A)</m> and therefore to solve least squares
	    problems.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>A</m> is a demeaned data matrix, the left singular
	    vectors give the principal components of <m>A</m> and the
	    variance in the direction of a principal component can be
	    easily expressed in terms of the corresponding singular
	    value.
	  </p>
	</li>

	<li>
	  <p>
	    The singular value decomposition has many applications.
	    In this section, we looked at how the decomposition is
	    used in image processing through the techniques of
	    compression and denoising.
	  </p>
	</li>

	<li>
	  <p>
	    Because the first few left singular vectors contain the
	    most important features of a matrix, we can use a singular
	    value decomposition to extract meaning from a large data
	    set as we did when analyzing the voting patterns of the
	    second Rehnquist court.
	  </p>
	</li>
      </ul>
    </p>


  </subsection>

<!--
      <exercises>

Do the rest of the SVD lab
agreement matrix
-->

</section>


