<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-svd-uses"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Using Singular Value Decompositions </title>

  <introduction>
    <p>
      We've now seen what singular value decompositions are, how to
      construct them, and how they provide orthonormal bases for the
      four fundamental subspaces associated to a matrix.  This puts us
      in a good position to begin using singular value decompositions
      to solve a wide variety of problems.
    </p>

    <p>
      Given the fact that singular value decompositions so immediately
      convey fundamental data about a matrix, such as its rank and
      column space, it seems natural that some of our previous work
      can be reinterpreted in terms of singular value decompositions.
      Therefore, we'll take some time in this section to revisit some
      familiar situations, such as least squares problems and principal
      component analysis, from the perspective of singular value
      decompositions.  We'll begin, however, by exploring how a
      singular value decomposition of <m>A</m> provides a sequence of
      matrices <m>A_k</m> that approximate <m>A</m>, an observation
      that will unlock some new and important applications.
    </p>

    <exploration>
      <statement>
	<p>
	  Suppose that <m>A = U\Sigma V^T</m> where
	  <me>
	    \Sigma = \begin{bmatrix}
	    13 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 8 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 2 \amp 0 \\
	    0 \amp0 \amp 0 \amp 0 \\
	    0 \amp0 \amp 0 \amp 0
	    \end{bmatrix},
	  </me>
	  vectors <m>\uvec_j</m> form the columns of <m>U</m>, and
	  vectors 
	  <m>\vvec_j</m> form the columns of <m>V</m>.
	  <ol label="a.">
	    <li>
	      <p>
		What is the shape of the matrices <m>A</m>, <m>U</m>,
		and <m>V</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		What is the rank of <m>A</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		Describe how to find an orthonormal basis for
		<m>\col(A)</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		Describe how to find an orthonormal basis for
		<m>\nul(A)</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		If the columns of <m>Q</m> form an orthonormal basis
		for <m>\col(A)</m>, what is <m>Q^TQ</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		How would you form a matrix that projects vectors
		orthogonally onto <m>\col(A)</m>?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </exploration>

  </introduction>

  <subsection>
    <title> Rank <m>k</m> approximations </title>

    <p>
      If we have a singular value decomposition for a matrix <m>A</m>,
      we can form a sequence of matrices <m>A_k</m> that approximate
      <m>A</m>.  Readers who are familiar with calculus will recognize
      a similarity to the way in which a
      function <m>f(x)</m> can be approximated by a linear function, a
      quadratic function, and so forth with increasing accuracy.
    </p>

    <p>
      To understand these approximations, 
      imagine that we have a singular value
      decomposition of the
      <m>m\times n</m> matrix <m>A=U\Sigma V^T</m> and that the rank
      of <m>A</m> is <m>r</m>.  If we write an <m>n</m>-dimensional
      vector <m>\xvec</m> as a linear combination of right singular
      vectors, we have
      <me>
	\xvec=c_1\vvec_1 + c_2\vvec_2 + \ldots + c_n\vvec_n.
      </me>
      Notice that <m>c_j = \vvec_n\cdot\xvec = \vvec_n^T\xvec</m>.
      We then have
      <md>
	<mrow>
	  A \xvec \amp = A(c_1\vvec_1 + c_2\vvec_2 + \ldots +
	  c_n\vvec_n)
	</mrow>
	<mrow>
	  \amp = c_1A\vvec_1 + c_2A\vvec_2 + \ldots + c_nA\vvec_n
	</mrow>
	<mrow>
	  \amp = c_1\sigma_1\uvec_1 + c_2\sigma_2\uvec_2 + \ldots
	  + c_r\sigma_r\uvec_r
	</mrow>
	<mrow>
	  \amp = \sigma_1\uvec_1\vvec_1^T\xvec +
	  \sigma_2\uvec_2\vvec_2^T\xvec + \ldots +
	  \sigma_r\uvec_r\vvec_r^T\xvec.
	</mrow>
      </md>
      Consequently, we find that
      <me>
	A = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + \ldots +
	  \sigma_r\uvec_r\vvec_r^T.
      </me>
      In other words, we can write <m>A</m> as a sum of matrices each
      of which has the form <m>\sigma_j\uvec_j\vvec_j^T</m>. 
    </p>

    <p>
      Remember that we have ordered the singular values so that
      <m>\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r\gt 0</m>.  This
      means that the first singular value is the largest one so we can
      expect that the first term in this sum
      <m>\sigma_1\uvec_1\vvec_1^T</m> is the most important, the
      second term <m>\sigma_2\uvec_2\vvec_2^T</m> the next most
      important term and so forth.  This
      leads us to define the series of approximating matrices
      <md>
	<mrow>
	  A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T
	</mrow>
	<mrow>
	  A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T
	</mrow>
	<mrow>
	  A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + 
	  \sigma_3\uvec_3\vvec_3^T
	</mrow>
	<mrow>
	  \vdots \amp
	</mrow>
	<mrow>
	  A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
	  \sigma_2\uvec_2\vvec_2^T + \ldots +
	  \sigma_r\uvec_r\vvec_r^T
	</mrow>
      </md>
    </p>

    <p>
      There is a simple way to form these approximations
      computationally from a singular value decomposition <m>A=U\Sigma
      V^T</m>.  Notice that the approximation <m>A_k</m> has the same
      singular vectors as <m>A</m> and same first <m>k</m> singular
      values.  Therefore, we can form <m>A_k</m> by modifying
      <m>\Sigma</m> so that the singular values
      beyond the first <m>k</m> are zero.  For instance, if
      <m>
	\Sigma = \begin{bmatrix}
	22 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 14 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 3 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	\end{bmatrix}
	</m>, we can form <m>A_2</m> by replacing <m>\Sigma</m> with
	<m>
	\begin{bmatrix}
	22 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 14 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	\end{bmatrix}
      </m>.
    </p>

    <activity>
      <p>
	Let's consider a matrix <m>A=U\Sigma V^T</m> where
	<md>
	  <mrow>
	    \amp U = \begin{bmatrix}
	    \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    \frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
	    \end{bmatrix},\hspace{10pt}
	    \Sigma = \begin{bmatrix}
	    500 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 100 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 20 \amp 0  \\
	    0 \amp 0 \amp 0 \amp 4
	    \end{bmatrix}
	  </mrow>
	  <mrow>
	    \amp V = \begin{bmatrix}
	    \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
	    -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
	    -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2}
	    \end{bmatrix}
	  </mrow>
	</md>
	Evaluating the following cell will create the matrices
	<c>U</c>, <c>V</c>, and <c>Sigma</c>.  Notice how the
	<c>diagonal_matrix</c> command provides a convenient way to
	form a diagonal 
	matrix. 
	<sage>
	  <input>
h = 1/2
U = matrix(4,4,[h,h,h,h,  h,h,-h,-h,  h,-h,h,-h,  h,-h,-h,h])	    
V = matrix(4,4,[h,h,h,h,  h,-h,-h,h,  -h,-h,h,h,  -h,h,-h,h])
Sigma = diagonal_matrix([500, 100, 20, 4])
	  </input>
	</sage>
	<ol label="a.">
	  <li>
	    <p>
	      Form the matrix <m>A=U\Sigma V^T</m>.  What is
	      <m>\rank(A)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Now form the approximating matrix <m>A_1=U\Sigma_1
	      V^T</m>.  What is <m>\rank(A_1)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Find the error in the approximation, which may be
	      expressed as
	      <m>A-A_1</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Now find <m>A_2 = U\Sigma_2 V^T</m> and the error
	      <m>A-A_2</m>.  What is <m>\rank(A_2)</m>?
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Find <m>A_3 = U\Sigma_3 V^T</m> and the error
	      <m>A-A_3</m>.  What is <m>\rank(A_3)</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      What would happen if we were to compute <m>A_4</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      What do you notice about the error <m>A-A_k</m> as
	      <m>k</m> increases?
	    </p>
	  </li>
	</ol>
      </p>
    </activity>

    <p>
      In this activity, we saw that the approximation <m>A_k</m> has
      rank <m>k</m> because its singular value decomposition has
      <m>k</m> nonzero singular values.  We also saw how the
      difference between <m>A</m> and the approximations <m>A_k</m>
      decrease as <m>k</m> increases, which means that the sequence <m>A_k</m>
      forms better approximations as <m>k</m> increases.  In fact,
      <m>A_k</m> is the matrix that is closest to <m>A</m>, in a sense
      that can be made precise, among all rank <m>k</m> matrices.
    </p>

    <p>
      There is an alternate way to write the approximating matrices
      <m>A_k</m> that is sometimes useful.  For example, suppose
      that some matrix has the singular value
      decomposition <m>U\Sigma V^T</m> where
      <me>
	U = \begin{bmatrix}
	\uvec_1 \amp \uvec_2 \amp \uvec_3
	\end{bmatrix},\hspace{10pt}
	\Sigma = \begin{bmatrix}
	5 \amp 0 \\
	0 \amp 3 \\
	0 \amp 0
	\end{bmatrix}.
      </me>
      We can define new matrices by trimming off
      the last column of <m>U</m> and the last row of
      <m>\Sigma</m>: 
      <me>U_2 = \begin{bmatrix}
      \uvec_1 \amp \uvec_2 
      \end{bmatrix},\hspace{10pt}
      \Sigma_2 = \begin{bmatrix}
      5 \amp 0 \\
      0 \amp 3 \\
      \end{bmatrix}.
      </me>
    </p>

    <p>	
      Remembering that the columns of the matrix product
      <m>U\Sigma</m> are formed by multiplying the columns of
      <m>\Sigma</m> by <m>U</m>, then we have
      <me>
	U\Sigma =
	\begin{bmatrix}
	\uvec_1\amp\uvec_2\amp\uvec_3
	\end{bmatrix}
	\begin{bmatrix}
	5 \amp 0 \\
	0 \amp 3 \\
	0 \amp 0
	\end{bmatrix}
	= \begin{bmatrix}
	5\uvec_1 \amp 3\uvec_2
	\end{bmatrix}
	= \begin{bmatrix}
	\uvec_1\amp\uvec_2
	\end{bmatrix}
	\begin{bmatrix}
	5 \amp 0 \\
	0 \amp 3 \\
	\end{bmatrix}
	= U_2\Sigma_2.
      </me>
    </p>

    <p>
      More generally, if we trim columns from <m>U</m> and <m>V</m>
      and form the square matrix <m>\Sigma_k</m> such that
      <me>
	U_k = \begin{bmatrix}
	\uvec_1 \amp \uvec_2 \amp \ldots \uvec_k
	\end{bmatrix},\hspace{10pt}
	\Sigma_k = \begin{bmatrix}
	\sigma_1 \amp 0 \amp \ldots \amp 0 \\
	0 \amp \sigma_2 \amp \ldots \amp 0 \\
	\vdots \amp \vdots \amp \ddots \amp \vdots \\
	0 \amp 0 \amp \ldots \amp \sigma_k
	\end{bmatrix},\hspace{10pt}
	V_k = \begin{bmatrix}
	\vvec_1 \amp \vvec_2 \amp \ldots \vvec_k
	\end{bmatrix},
      </me>
      then we have <m>A_k = U_k\Sigma_kV_k^T</m>.
    </p>

    <p>
      If the rank of <m>A</m> is <m>r</m>, then we have
      <m>A=U_r\Sigma_rV_r</m>, which is called a
      <em>reduced</em> singular value decomposition of <m>A</m>.  As
      we'll see, this can be useful because the matrix <m>\Sigma_r</m>
      is a square, invertible matrix.
    </p>

  </subsection>
      
  <subsection>
    <title> Least squares problems </title>

    <p>
      Least squares problems, which we explored in <xref
      ref="sec-least-squares" />, arise when we are confronted with an
      inconsistent linear system <m>A\xvec=\bvec</m>.  Since there is
      no solution to the system, we instead find the vector
      <m>\xvec</m> minimizing the distance between <m>\bvec</m>
      and <m>A\xvec</m>. 
      That is, we find the
      vector <m>\xhat</m>, the least squares approximate solution, by
      solving <m>A\xhat=\bhat</m> where <m>\bhat</m> is the orthogonal
      projection of <m>\bvec</m> onto the column space of <m>A</m>.
    </p>

    <p>
      If we have a decomposition <m>A=U\Sigma V^T</m>, then the number
      of nonzero singular values <m>r</m> tells us the rank of
      <m>A</m>, and the first <m>r</m> columns of <m>U</m> form an
      orthonormal basis for <m>\col(A)</m>.  This basis may be used to
      project vectors onto <m>\col(A)</m> and hence to solve least
      squares problems.
    </p>

    <p>
      Before exploring this connection further, we will introduce Sage 
      as a tool for automating the construction of singular value
      decompositions.  One new feature is that we need to declare our
      matrix to consist of floating point entries.  We do this by
      including <c>RDF</c> inside the matrix definition, as
      illustrated in the following cell.
      <sage>
	<input>
A = matrix(RDF, 3, 2, [1,0,-1,1,1,1])
U, Sigma, V = A.SVD()	  
print(U)
print(Sigma)
print(V)
	</input>
      </sage>
    </p>

    <activity>
      <statement>
	<p>
	  Consider the equation <m>A\xvec=\bvec</m> where
	  <me>
	    \begin{bmatrix}
	    1 \amp 0 \\
	    1 \amp 1 \\
	    1 \amp 2
	    \end{bmatrix}
	    \xvec = \threevec{-1}36
	  </me>

	  <ol label="a.">
	    <li>
	      <p>
		Find a singular value decomposition for <m>A</m> using
		the Sage cell below.  What are singular values of
		<m>A</m>?
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	    <li>
	      <p>
		What is <m>r</m>, the rank of <m>A</m>?  How can we
		identify an orthonormal basis for <m>\col(A)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose we write <m>\bvec</m> as a linear combination
		of the columns of <m>U</m>:
		<me>
		  \bvec = c_1\uvec_1 + c_2\uvec_2 + c_3\uvec_3.
		</me>
		<ol label="i.">
		  <li>
		    <p>
		      What condition on the weights <m>c_1</m>, <m>c_2</m>,
		      and <m>c_3</m> must be satisfied if <m>A\xvec =
		      \bvec</m> 
		      is consistent?
		    </p>
		  </li>
		  <li>
		    <p>
		      Similarly, what condition on <m>U^T\bvec</m>
		      must be satisfied if <m>A\xvec=\bvec</m> is
		      consistent?
		    </p>
		  </li>
		  <li>
		    <p>
		      Evaluate <m>U^T\bvec</m> to determine if
		      <m>A\xvec=\bvec</m> is consistent.
		      <sage>
			<input>
			</input>
		      </sage>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Form the reduced singular value decomposition
		<m>U_r\Sigma_rV_r^T</m> by constructing the matrix
		<m>U_r</m>, consisting of 
		the first <m>r</m> columns of <m>U</m>,
		the matrix <m>V_r</m>, 
		consisting of the first <m>r</m> columns of <m>V</m>,
		and <m>\Sigma_r</m>, an <m>r\times r</m> diagonal matrix.
		Verify that <m>A=U_r\Sigma_r V_r^T</m>.
	      </p>
	      <p>
		You may find it convenient to remember that, if
		<c>B</c> is a matrix defined in Sage, then
		<c>B.matrix_from_columns( list )</c> and
		<c>B.matrix_from_rows( list )</c> can be used to
		extract columns or rows from <c>B</c>.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		How does the reduced singular value decomposition
		provide a matrix whose columns are an orthonormal basis
		for <m>\col(A)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Explain why <m>\xhat</m>, the least squares
		approximate solution, satisfies
		<me>
		  A\xhat = U_r\Sigma_rV_r^T\xhat = U_rU_r^T\bvec.
		</me>
	      </p>
	    </li>

	    <li>
	      <p>
		What is the product <m>U_r^TU_r</m> and why does it
		have this form?
	      </p>
	    </li>

	    <li>
	      <p>
		Explain why
		<me>
		  \xhat = V_r\Sigma_r^{-1}U_r^T\bvec
		</me>
		and use this expression to find <m>\xhat</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	      
	  </ol>
	</p>
      </statement>
    </activity>
		
    <p>
      Using the singular value decomposition <m>A=U\Sigma V^T</m>
      provided by Sage in the activity, 
      we see that <m>\rank(A)=2</m> because there are
      two nonzero singular values.  This means that the column space
      is two-dimensional and that <m>\uvec_1</m> and
      <m>\uvec_2</m>, the first two columns of <m>U</m>, form an
      orthonormal basis for <m>\col(A)</m>.
    </p>

    <p>
      If we are given a three-dimensional vector <m>\bvec</m>, we may
      write it as a linear cominbation of the columns of <m>U</m>;
      that is, <m>\bvec=c_1\uvec_1+c_2\uvec_2+c_3\uvec_3</m>.  Since
      <m>\uvec_1</m> and <m>\uvec_2</m> form a basis for <m>
      \col(A)</m>, we see that <m>A\xvec=\bvec</m> is only consistent
      if <m>c_3 = 0</m>.  Notice that
      <me>
	U^T\bvec = \threevec{\uvec_1\cdot\bvec}
	{\uvec_2\cdot\bvec}
	{\uvec_3\cdot\bvec}
	=\threevec{c_1}{c_2}{c_3},
      </me>
      which implies that <m>A\xvec = \bvec</m> is consistent if and
      only if the
      third component of <m>U^T\bvec</m> is zero.
    </p>

    <p>
      Since the rank <m>r</m> of <m>A</m> is 2, the reduced singular
      value decomposition is <m>A=U_2\Sigma_2V_2^T</m> where
      <me>
	U_2=\begin{bmatrix} \uvec_1\amp\uvec_2 \end{bmatrix},
	\hspace{10pt}
	\Sigma_2 = \begin{bmatrix}
	\sigma_1 \amp 0 \\
	0 \amp \sigma_2
	\end{bmatrix},\hspace{10pt}
	V_2 = \begin{bmatrix}
	\vvec_1\amp\vvec_2
	\end{bmatrix} = V.
      </me>
      Notice that the columns of <m>U_2</m> form an orthonormal basis
      for <m>\col(A)</m>.  Therefore, the vector <m>\bhat</m> that
      results from projecting <m>\bvec</m> onto <m>\col(A)</m> is
      <m>\bhat = U_2U_2^T\bvec</m>.  In other words, we have
      <md>
	<mrow>
	  A\xhat \amp = \bhat
	</mrow>
	<mrow>
	  U_2\Sigma_2V_2^T \xhat \amp = U_2U_2^T\bvec.
	</mrow>
      </md>
    </p>

    <p>
      Remembering that <m>U_2^TU_2=I_2</m>, the <m>2\times2</m>
      identity, and multiplying by <m>U_2^T</m>, we have
      <md>
	<mrow>
	  U_2^T ~U_2\Sigma_2V_2^T\xhat \amp = U_2^T~U_2U_2^T\bvec
	</mrow>
	<mrow>
	  \Sigma_2V_2^T\xhat \amp = U_2^T\bvec
	</mrow>
	<mrow>
	  V_2^T\xhat\amp = \Sigma_2^{-1} U_2^T\bvec
	</mrow>
	<mrow>
	  \xhat\amp = V_2\Sigma_2^{-1} U_2^T\bvec.
	</mrow>
      </md>
    </p>

    <proposition xml:id="prop-svd-ols">
      <statement>
	<p>
	  If <m>A=U_r\Sigma_r V_r^T</m> is a reduced singular value
	  decomposition of <m>A</m>, then a least squares approximate
	  solution to <m>A\xvec=\bvec</m> is given by
	  <me>
	    \xhat = V_r\Sigma_r^{-1}U_r^T\bvec.
	  </me>
	</p>
      </statement>
    </proposition>

    <p>
      If the columns of <m>A</m> are linearly independent, then the
      equation <m>A\xhat = \bhat</m> has only one solution so there is
      a unique least squares approximate solution <m>\xhat</m>.
      Otherwise, the expression in <xref ref="prop-svd-ols"/> produces
      the solution to <m>A\xhat=\bhat</m> having the shortest
      length. 
    </p>

    <p>
      We have now seen several ways to find an approximate least
      squares solution to an equation <m>A\xvec=\bvec</m>.  First, we
      can find <m>\bhat</m>, the orthogonal projection of
      <m>\bvec</m> onto <m>\col(A)</m>, and directly solve the
      equation <m>A\xhat = \bhat</m>.  A <m>QR</m> factorization of
      <m>A</m> provides a second method, and a singular value
      decomposition of <m>A</m> now gives us a third.
    </p>

  </subsection>

  <subsection>
    <title> Principal component analysis </title>

    <p>
      In <xref ref="sec-pca" />, we explored principal component
      analysis as a technique to reduce the dimension of a dataset.
      In particular, we constructed the covariance matrix <m>C</m>
      from a demeaned data matrix and saw that the eigenvalues and
      eigenvectors of <m>C</m> gave us a way to understand the
      variance of the dataset in different directions.  We called
      these eigenvectors <em>principal components</em> and found that
      projecting the data onto the principal components having the
      largest associated eigenvalues frequently gave us a way to
      visualize the dataset.
    </p>

    <p>
      Suppose that we have a dataset with <m>N</m> points,
      that <m>A</m> represents the demeaned data matrix, that
      <m>A = U\Sigma V^T</m> is a singular value decomposition, and
      that the singular values are <m>A</m> are denoted as
      <m>\sigma_j</m>. 
      It follows that the covariance matrix
      <me>
	C = \frac1N AA^T = \frac1N (U\Sigma V^T) (U\Sigma V^T)^T =
	U\left(\frac1N \Sigma \Sigma^T\right) U^T.
      </me>
      Notice that <m>\frac1N \Sigma\Sigma^T</m> is a diagonal matrix
      whose diagonal entries are <m>\frac1N\sigma_j^2</m>.  Therefore,
      it follows that 
      <me>
	C = 
	U\left(\frac1N \Sigma \Sigma^T\right) U^T
      </me>
      is an orthgonal diagonalization of <m>C</m> showing that
      <ul>
	<li>
	  <p>
	    the principal components of the dataset, which are the
	    eigenvectors of <m>C</m>, are given by the columns
	    of <m>U</m>.  In other words, the left singular vectors of
	    <m>A</m> are the principal components of the
	    dataset.
	  </p>
	</li>
	<li>
	  <p>
	    the variance in the direction of a principal
	    component is the associated eigenvalue of <m>C</m> and 
	    therefore 
	    <me>
	      V_{\uvec_j} = \frac1N\sigma_j^2.
	    </me>
	  </p>
	</li>
      </ul>
    </p>

    <activity>
      <statement>
	<p>
	  Let's revisit the iris data set that we studied in
	  <xref ref="sec-pca" />.  Remember that there are four
	  measurements given for each of 150 irises.  The irises
	  are grouped into three species of which there are 50
	  each.
	</p>

	<p>
	  Evaluating the following cell will load the dataset and
	  define the demeaned
	  data matrix <m>A</m> whose shape is <m>4\times150</m>.
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/pca-iris.py', globals())
df.T

	    </input>
	  </sage>
	  
	  <ol label="a.">
	    <li>
	      <p>
		Find the singular values of <m>A</m> using the command
		<c>A.singular_values()</c> and use them to determine
		the variance <m>V_{\uvec_j}</m> in the direction of
		each of the four principal components.  What is the
		fraction of variance explained by the first two
		principal components?
		<sage>
		  <input>


		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		We will now write the matrix <m>\Gamma = \Sigma
		V^T</m> so that <m>A = U\Gamma</m>.
		Suppose that a demeaned data point, say, the 100th
		column of <m>A</m>, is written as a
		linear combination of principal components:
		<me>
		  \xvec = c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4.
		</me>
		Explain why
		<m>\fourvec{c_1}{c_2}{c_3}{c_4}</m>,
		the vector of coordinates of <m>\xvec</m> in the
		basis of principal components,
		appears as 100th
		column of <m>\Gamma</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that we now project this demeaned data point
		<m>\xvec</m> orthogonally onto the subspace spanned by
		the first two principal components <m>\uvec_1</m> and
		<m>\uvec_2</m>.  What are the coordinates of the
		projected point in this basis and how can we find them
		in the matrix <m>\Gamma</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Alternatively, consider the approximation
		<m>A_2=U_2\Sigma_2V_2^T</m> of the demeaned data
		matrix <m>A</m>.  
		Explain why the 100th column of
		<m>A_2</m> represents the projection of <m>\xvec</m>
		onto the two-dimensional subspace spanned by the first
		two principal components, <m>\uvec_1</m> and
		<m>\uvec_2</m>.  Then explain why the coefficients in
		that projection, <m>c_1\uvec_1 + c_2\uvec_2</m>, form
		the two-dimensional vector <m>\twovec{c_1}{c_2}</m> that
		is the 100th column of <m>\Gamma_2=\Sigma_2
		V_2^T</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Now we've seen that the columns of <m>\Gamma_2 =
		\Sigma_2 V_2^T</m> form the coordinates of the
		demeaned data points projected on to the two-dimensional
		subspace spanned by <m>\uvec_1</m> and <m>\uvec_2</m>.
		In the cell below, find a singular value decomposition
		of <m>A</m> and use it to form the matrix
		<c>Gamma2</c> below.  When you 
		evaluate this cell, you will see a plot of the
		projected demeaned data plots, similar to the one we
		created in <xref ref="sec-pca" />.
		<sage>
		  <input>
# Form the SVD of A and use it to form Gamma2

		    
Gamma2 = 

# The following will plot the projected demeaned data points
data = Gamma2.columns()
(list_plot(data[:50], color='blue', aspect_ratio=1) +
 list_plot(data[50:100], color='orange') +
 list_plot(data[100:], color='green'))
		  </input>
		</sage>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

    <p>
      This activity demonstrates the connection between singular value
      decompositions and principal component analysis.  In particular,
      we have seen that the principal components of a dataset
      <m>\uvec_j</m> are the 
      left singular vectors of the demeaned data matrix <m>A</m> and
      that the variance in the direction of a principal component
      is determined by
      the singular values of <m>A</m>,
      <me>
	V_{\uvec_j} = \frac{\sigma_j^2}{N}.
      </me>
      Applying this
      to the iris dataset, we see that 97.8% of the variance in the
      data is explained by the first two principal components.  This
      agrees with 
      the result we found in <xref ref="sec-pca"/>.
    </p>

    <p>
      If a data
      point <m>\xvec</m> is represented as a linear combination of
      principal components
      <me>
	\xvec=c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4,
      </me>
      then the projection formula tells us that the projection of
      <m>\xvec</m> onto the first two principal components is
      <m>c_1\uvec_1 + c_2\uvec_2</m>.  The demeaned data point
      <m>\xvec</m> appears as a column in the data matrix <m>A</m> and
      its projection as the corresponding column in <m>A_2</m>.  Since
      we have
      <me>
	A_2=U_2\Sigma_2V_2^T = U_2\Gamma_2 =
	\begin{bmatrix}
	\uvec_1 \amp \uvec_2
	\end{bmatrix} \Gamma_2,
      </me>
      then the
      coordinate vector <m>\twovec{c_1}{c_2}</m> appears as the
      corresponding column of <m>\Gamma_2</m>.  To construct a plot of
      the points projected onto the first two principal components, we
      simply need to plot the points represented by the
      columns of <m>\Gamma_2</m>.
    </p>

    <p>
      In our first encounter with principal component analysis, we
      began with a demeaned data matrix <m>A</m>, formed the
      covariance matrix <m>C</m>, and used the eigenvalues and
      eigenvectors of <m>C</m> to project the demeaned data onto a
      smaller dimensional subspace.  In this section, we have seen
      that a singular value decomposition of <m>A</m> provides a more
      direct route: the left singular vectors of <m>A</m> form the
      principal components and the approximating matrix <m>A_k</m>
      represents the data points projected onto the subspace spanned
      by the first <m>k</m>
      principal components.  The coordinates of a projected demeaned
      data point are given by the columns of <m>\Gamma_k =
      \Sigma_kV_k^T</m>. 
    </p>

  </subsection>

  <subsection>
    <title> Image compressing and denoising </title>

    <p>
      In addition to principal component analysis, the approximations
      <m>A_k</m> of a matrix <m>A</m> obtained from 
      a singular value decomposition can be used in image processing.
      Remember that 
      we studied the JPEG compression
      algorithm, whose foundation is the change of basis defined by
      the Discrete Cosine Transform, in <xref ref="sec-jpeg" />.
      We will now see how a singular
      value decomposition provides another tool for both compressing
      images and removing noise in them.
    </p>

    <activity>
      <statement>
	<p>
	  Evaluating the following cell loads some data that we'll use
	  in this activity.  To begin, it defines and displays a
	  <m>25\times15</m> matrix <m>A</m>.
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/svd-compress.py', globals())
print(A)


	    </input>
	  </sage>
	  <ol label="a.">
	    <li>
	      <p> 
		If we interpret 0 as black and 1 as white, this matrix
		represents an image as shown below.
		<sage>
		  <input>
display_matrix(A)

		  </input>
		</sage>
		We will explore how the singular value
		decomposition helps us to compress this image.
		<ol label="i.">
		  <li>
		    <p>
		      By inspecting the image represented by <m>A</m>,
		      identify a basis for <m>\col(A)</m> and
		      determine <m>\rank(A)</m>.
		    </p>
		  </li>

		  <li>
		    <p>
		      The following cell plots the singular values of
		      <m>A</m>.  Explain how this plot verifies that
		      the rank is what you found in the previous part.
		      <sage>
			<input>
plot_sv(A)		    

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      There is a command <c>approximate(A, k)</c> that
		      creates the approximation <m>A_k</m>.  Use the
		      cell below to define <m>k</m> and look at the
		      images represented by the
		      first few approximations.  What is the smallest
		      value of <m>k</m> for which <m>A=A_k</m>?
		      <sage>
			<input>
k = 
display_matrix(approximate(A, k))			  
			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Now we can see how the singular value
		      decomposition allows us to compress images.
		      Since this is a <m>25\times15</m> matrix, we need
		      <m>25\cdot15=375</m> numbers to
		      represent the image.  However, we can also
		      reconstruct the image using a small number of
		      singular values and vectors:
		      <me>
			A = A_k = \sigma_1\uvec_1\vvec_1^T +
			\sigma_2\uvec_2\vvec_2^T + \ldots +
			\sigma_k\uvec_k\vvec_k^T.
		      </me>
		      What are the dimensions of the singular vectors
		      <m>\uvec_i</m> and <m>\vvec_i</m>?  Between the
		      singular vectors and singular values, how many
		      numbers do we need to reconstruct
		      <m>A_k</m> for the smallest <m>k</m> for which
		      <m>A=A_k</m>?  This is the compressed size of
		      the image.
		    </p>
		  </li>

		  <li>
		    <p>
		      The <em>compression ratio</em> is
		      the ratio of the uncompressed size to the
		      compressed size.  What compression ratio does 
		      this represent?
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Next we'll explore an example based on a photograph. 
		<ol label="i">
		  <li>
		    <p>
		      Consider the following image consisting of an
		      array of <m>316\times310</m> pixels stored in the
		      matrix <m>A</m>.
		      <sage>
			<input>
A = matrix(RDF, image)
display_image(A)

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Plot the singular values of <m>A</m>.
		      <sage>
			<input>
plot_sv(A)
			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Use the cell below to study the approximations
		      <m>A_k</m> for <m>k=1, 10, 20, 50, 100</m>.
		      <sage>
			<input>
k = 1
display_image(approximate(A, k))		    

			</input>
		      </sage>
		      Notice how the approximating image
		      <m>A_k</m> more closely approximates the
		      original image <m>A</m> as <m>k</m> increases.
		    </p>
		    <p>
		      What is the compression ratio when <m>k=50</m>?
		      What is the compression ratio when <m>k=100</m>?
		      Notice how a higher compression ratio leads to a
		      lower quality reconstruction of the image.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		A second, related application of the singular value
		decomposition to image processing is called
		<em>denoising</em>.  For example, consider the image
		represented by the matrix <m>A</m> below.
		<sage>
		  <input>
A = matrix(RDF, noise.values)		    
display_matrix(A)		
		  </input>
		</sage>
		This image is similar to the image of the letter "O"
		we first studied in this activity, but there are
		splotchy regions in the background that result,
		perhaps, from scanning the image.  We think of the
		splotchy regions as noise, and our goal is to improve
		the quality of the image by reducing the noise.

		<ol label="i.">
		  <li>
		    <p>
		      Plot the singular values below.  How are the
		      singular values of this matrix similar to those
		      represented by the clean image that we 
		      considered earlier and how are they different?
		      <sage>
			<input>
plot_sv(A)

			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      There is a natural point where the singular
		      values dramatically decrease so it makes sense
		      to think of the noise as being formed by the
		      small singular values.  To denoise the image, we
		      will therefore replace <m>A</m> by its
		      approximation <m>A_k</m>, where <m>k</m> is the
		      point at which the singular values drop off.
		      This has the effect of
		      setting the small singular values to zero and
		      hence eliminating the noise.
		      Choose an appropriate value of <m>k</m> below
		      and notice that the new image appears to be
		      cleaned up as a result of removing the noise.
		      <sage>
			<input>
k = 
display_matrix(approximate(A, k))

			</input>
		      </sage>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

  </subsection>

  <subsection>
    <title>
      Analyzing Supreme Court cases
    </title>

    <p>
      As we've seen, a singular value decomposition concentrates the
      most important features of a matrix into the first singular
      values and singular vectors.  We will now use this observation
      to extract meaning from a large dataset giving the
      voting records of Supreme Court justices.  A similar analysis
      appears in the paper <url
      href="https://www.pnas.org/content/100/13/7432"> A pattern
      analysis of the second Rehnquist U.S. Supreme Court </url> by
      Lawrence Sirovich.  
    </p>

    <p>
      The makeup of the Supreme Court was unusually stable during a
      period from 1994-2005 when it was led by Chief Justice William
      Rehnquist.  This is sometimes called the <em>second Rehnquist
      court</em>.  The justices during this period were:
      <ul>
	<li><p> William Rehnquist </p></li>
	<li><p> Antonin Scalia </p></li>
	<li><p> Clarence Thomas </p></li>
	<li><p> Anthony Kennedy </p></li>
	<li><p> Sandra Day O'Connor </p></li>
	<li><p> John Paul Stevens </p></li>
	<li><p> David Souter </p></li>
	<li><p> Ruth Bader Ginsburg </p></li>
	<li><p> Stephen Breyer </p></li>
      </ul>
    </p>

    <p>
      During this time, there were 911 cases in which all nine
      judges voted.  We would like to understand patterns in their
      voting. 
    </p>

    <activity>
      <statement>
	<p>
	  Evaluating the following cell loads and displays a dataset
	  describing the votes of each justice in these 911 cases.
	  More specifically, an entry of +1 means the justice
	  represented by the row voted with the majority in the case
	  represented by the column.  An entry of -1 means that
	  justice was in the minority.  This information is also
	  stored in the <m>9\times911</m> matrix <m>A</m>.
	  
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/svd-supreme.py', globals())
A = matrix(RDF, cases.values)
cases

	    </input>
	  </sage>
	  The justices are listed, very roughly, in order from more
	  conservative to more progressive.
	</p>

	<p>
	  In this activity, it will be helpful to visualize the
	  entries in various matrices and vectors.  The next cell
	  displays the matrix <m>A</m> with white
	  representing an entry of +1, red representing -1, and black
	  representing 0. 
	  <sage>
	    <input>
display_matrix(A.matrix_from_columns(range(50)))


	    </input>
	  </sage>
	  <ol label="a.">
	    <li>
	      <p>
		Plot the singular values of <m>A</m> below.  Describe
		the significance of this plot, including the relative
		contributions from the singular values <m>\sigma_k</m>
		as <m>k</m> increases.
		<sage>
		  <input>
plot_sv(A)
		  </input>
		</sage>
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Form the singular value decomposition <m>A=U\Sigma
		V^T</m> and the matrix of coefficients <m>\Gamma</m>
		so that <m>A=U\Gamma</m>.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		We will now study a particular case, the second case
		appearing as the column of <m>A</m> indexed
		by <c>1</c>.
		There is a command <c>display_column(A, k)</c> that
		provides a visual display of the <m>k^{th}</m> column
		of a matrix <m>A</m>.  Describe the justices' votes in
		the second case.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Also, display the first left singular vector
		<m>\uvec_1</m>, the column of 
		<m>U</m> indexed by <m>0</m>, and the column of
		<m>\Gamma</m> holding the coefficients that express the
		second case as a linear combination of left singular
		vectors.
		<sage>
		  <input>

		  </input>
		</sage>

		What does this tell us about how the second case is
		constructed as a linear combination of left singular
		vectors?  What is the significance of the first left
		singular vector <m>\uvec_1</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Let's now study the <m>48^{th}</m> case, which is
		represented by the column of <m>A</m> indexed by
		<c>47</c>.  Describe the voting pattern in this case.
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Display the second left singular vector <m>\uvec_2</m>
		and the vector 
		of coefficients that express the <m>48^{th}</m> case as
		a linear combination of left singular vectors.
		<sage>
		  <input>

		  </input>
		</sage>
		Describe how this case is constructed as a linear
		combination of singular vectors.  What is the
		significance of the second left singular vector
		<m>\uvec_2</m>? 
	      </p>
	    </li>

	    <li>
	      <p>
		The data in <xref ref="table-supreme-cases" /> describes
		the number of cases decided by a particular count of
		votes. 
		<table xml:id="table-supreme-cases">
		  <title> Number of cases by vote count </title>
		  <tabular halign="center">
		    <row bottom="minor">
		      <cell> Vote count </cell>
		      <cell> # of cases </cell>
		    </row>
		    <row>
		      <cell> 9-0 </cell>
		      <cell> 405 </cell>
		    </row>
		    <row>
		      <cell> 8-1 </cell>
		      <cell> 89 </cell>
		    </row>
		    <row>
		      <cell> 7-2 </cell>
		      <cell> 111 </cell>
		    </row>
		    <row>
		      <cell> 6-3 </cell>
		      <cell> 118 </cell>
		    </row>
		    <row>
		      <cell> 5-4 </cell>
		      <cell> 188 </cell>
		    </row>
		  </tabular>
		</table>
		How do the singular vectors <m>\uvec_1</m> and
		<m>\uvec_2</m> reflect this data?  Would you
		characterize the court as leaning toward the
		conservatives or progressives?  Use these singular
		vectors to explain your response.
	      </p>
	    </li>

	    <li>
	      <p>
		Cases decided by a 5-4 vote are often the most
		impactful as they represent a sharp divide among the
		justices and, often, society at large.  For that
		reason, we will now focus on the 5-4 decisions.
		Evaluating the next cell forms the <m>9\times188</m>
		matrix <m>A</m> consisting of 5-4 decisions.
		<sage>
		  <input>
A = matrix(RDF, fivefour.values)
display_matrix(A.matrix_from_columns(range(50)))

		  </input>
		</sage>
		Form the singular value decomposition of <m>A=U\Sigma
		V^T</m> 
		along with the matrix <m>\Gamma</m> of coefficients so
		that <m>A=U\Gamma</m> and display the first left
		singular vector <m>\uvec_1</m>.  Study how the
		<m>7^{th}</m> case, indexed by <c>6</c>, is
		constructed as a linear 
		combination of left singular vectors.
		<sage>
		  <input>

		  </input>
		</sage>
		What does this singular vector tell us about the
		make up of the court and whether it leans towards the
		conservatives or progressives?  
	      </p>
	    </li>

	    <li>
	      <p>
		Display the second left singular vector
		<m>\uvec_2</m> and study how the <m>6^{th}</m> case,
		indexed by <c>5</c>, is constructed as a linear
		combination of left singular vector.
		<sage>
		  <input>

		  </input>
		</sage>
		What does <m>\uvec_2</m> tell us about the
		relative importance of the justices' voting records?
	      </p>
	    </li>

	    <li>
	      <p>
		By a <em>swing vote</em>, we mean a justice who is
		less inclined to vote with a particular bloc of
		justices but instead swings from one bloc to
		another with the potential to sway close decisions.
		What do the singular vectors <m>\uvec_1</m> and
		<m>\uvec_2</m> tell us about the presence of voting
		blocs on the court and and presence of a swing vote?
		Which justice represents the swing vote?
	      </p>
	    </li>
		
	  </ol>
	</p>

	
      </statement>
    </activity>
  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section has demonstrated some uses of the singular value
      decomposition.  Because the singular values appear in decreasing
      order, the decomposition has the effect of concentrating
      the most important features of the matrix into the first
      singular values and singular vectors.
      <ul>
	<li>
	  <p>
	    A singular value decomposition of a rank <m>r</m> matrix
	    <m>A</m> leads to a 
	    series of approximations <m>A_k</m> of <m>A</m> where 
	    <md>
	      <mrow>
		A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T
	      </mrow>
	      <mrow>
		A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T
	      </mrow>
	      <mrow>
		A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T + 
		\sigma_3\uvec_3\vvec_3^T
	      </mrow>
	      <mrow>
		\vdots \amp
	      </mrow>
	      <mrow>
		A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
		\sigma_2\uvec_2\vvec_2^T + \ldots +
		\sigma_r\uvec_r\vvec_r^T
	      </mrow>
	    </md>
	    In each case, <m>A_k</m> is the rank <m>k</m> matrix that
	    is closest to <m>A</m>.
	  </p>
	</li>

	<li>
	  <p>
	    Because the first left singular vectors form an
	    orthonormal basis for <m>\col(A)</m>, a singular value
	    decomposition provides a convenient way to project vectors
	    onto <m>\col(A)</m> and therefore to solve least squares
	    problems.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>A</m> is a demeaned data matrix, the left singular
	    vectors give the principal components of <m>A</m> and the
	    variance in the direction of a principal component can be
	    easily expressed in terms of the corresponding singular
	    value.
	  </p>
	</li>

	<li>
	  <p>
	    The singular value decomposition has many applications.
	    In this section, we looked at how the decomposition is
	    used in image processing through the techniques of
	    compression and denoising.
	  </p>
	</li>

	<li>
	  <p>
	    Because the first few left singular vectors contain the
	    most important features of a matrix, we can use a singular
	    value decomposition to extract meaning from a large dataset
	    as we did when analyzing the voting patterns of the
	    second Rehnquist court.
	  </p>
	</li>
      </ul>
    </p>


  </subsection>

  <xi:include href="exercises/exercises7-5.xml" />

</section>


