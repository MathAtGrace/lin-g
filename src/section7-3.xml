<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-pca"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Principal Component Analysis </title>

  <introduction>

    <p>
      As previously seen, we are sometimes presented with a dataset
      having quite a few data points that live in a high dimensional
      space.  For instance, we looked at a dataset describing
      the body fat index (BFI) in <xref ref="activity-BFI" /> where
      each data point is six-dimensional.  In this case,
      developing an intuitive understanding of the data is hampered by
      the fact that it cannot be visualized.
    </p>

    <p>
      This section explores a technique called <em> principal
      component analysis</em>, which enables us to reduce the
      dimension of a dataset so that it may be visualized or studied
      in some way that causes interesting features to stand out.  Our
      previous work with variance and the orthogonal diagonalization
      of symmetric matrices provides the key ideas.
    </p>

    <exploration>
      <statement>
	<p>
	  We will begin by recalling our earlier discussion of
	  variance.  Suppose we have a dataset that leads to the
	  covariance matrix
	  <me>
	    C = \begin{bmatrix}
	    7 \amp -4 \\
	    -4 \amp 13
	    \end{bmatrix}.
	  </me>
	  <ol label="a.">
	    <li>
	      <p>
		What is the variance in the direction <m>\uvec =
		\twovec10</m>;  that is, what is the variance of the
		demeaned data when projected onto the line defined by
		<m>\uvec</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		What is the variance in the direction
		<m>\uvec=\twovec01</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		What is the total variance?
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>\uvec</m> is an eigenvalue of <m>C</m>
		with eigenvalue <m>\lambda</m>.  What is the variance
		in the <m>\uvec</m> direction?
	      </p>
	    </li>

	    <li>
	      <p>
		Find an orthogonal diagonalization of <m>C</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		In
		which direction is the variance greatest and what is
		the variance in this direction?  If we project the
		data onto this line, how much variance is lost?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </exploration>

    <p>
      There are some ideas from our previous work that will be
      particularly useful
      here.  If <m>A</m> is a matrix of <m>N</m> demeaned data
      points, we form the covariance matrix <m>C=\frac 1N AA^T</m>.
      If <m>\uvec</m> is a unit vector, then the variance of the
      demeaned data after projecting onto the line defined by
      <m>\uvec</m> is given by the quadratic form <m>V_{\uvec} =
      \uvec\cdot(C\uvec)</m>.  In particular, if <m>\uvec</m> is an
      eigenvector of <m>C</m> with eigenvalue <m>\lambda</m>, then
      <m>V_{\uvec} = \lambda</m>.
    </p>

    <p>
      Moreover, variance is additive, as we recorded in <xref
      ref="prop-variance-additivity" />:  if <m>W</m> is a subspace
      having orthonormal basis <m>\uvec_1,\uvec_2,\ldots,\uvec_n</m>,
      then the variance
      <me>
	V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots + V_{\uvec_n}.
      </me>
    </p>

  </introduction>

  <subsection>
    <title> Principal Component Analysis </title>

    <p>
      Let's begin by looking at example that illustrates the central
      theme of this technique.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose that we work with a dataset having 100
	  five-dimensional data points.  The demeaned data matrix
	  <m>A</m> is therefore <m>5\times100</m> and leads to the
	  covariance matrix <m>C=\frac1{100}~AA^T</m>, which is
	  <m>5\times5</m>.  Because <m>C</m> is symmetric, the
	  Spectral Theorem tells us
	  it is orthogonally diagonalizable so suppose that <m>C =
	  QDQ^T</m> where
	  <me>
	    Q = \begin{bmatrix}
	    \uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4 \amp \uvec_5
	    \end{bmatrix},\hspace{24pt}
	    D = \begin{bmatrix}
	    13 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 10 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 2 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0
	    \end{bmatrix}.
	  </me>
	  <ol label="a.">
	    <li>
	      <p>
		What is <m>V_{\uvec_2}</m>, the variance in the
		<m>\uvec_2</m> direction?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Find the variance of the data projected onto the line
		defined by <m>\uvec_4</m>.  What does this say about the
		data?
	      </p>
	    </li>

	    <li>
	      <p>
		What is the total variance of the data?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Consider the two-dimensional subspace spanned by
		<m>\uvec_1</m> and <m>\uvec_2</m>.  If we project the
		data onto this subspace, what fraction 
		of the total variance is represented by the variance of the
		projected data?
	      </p>
	    </li>
	    
	    <li>
	      <p>	    
		How does this question change if we project onto the
		three-dimensional subspace 
		spanned by <m>\uvec_1</m>, <m>\uvec_2</m>, and
		<m>\uvec_3</m>?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		What does this tell us about the data?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

    <p>
      This activity illustrates how the eigenvalues of the covariance
      matrix tells us when data is clustered around a smaller
      dimensional subspace.  In particular, the original data is
      five-dimensional, but we see that it actually lies on a
      three-dimensional subspace of <m>\real^5</m>.  Later in this
      section, we'll see how to use this observation to work with the
      data as if it were three-dimensional, an idea known as
      <em>dimensional reduction</em>.
    </p>

    <p>
      To be more specific, remember that the variance in the direction
      of an eigenvector of the covariance matrix <m>C</m> equals its
      associated eigenvalue.  Now consider the sequence of subspaces
      <ul>
	<li>
	  <p>
	    <m>W_1</m>, the one-dimensional subspace spanned by
	    <m>\uvec_1</m>.
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_2</m>, the two-dimensional subspace spanned by
	    <m>\uvec_1</m> and <m>\uvec_2</m>.
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_3</m>, the three-dimensional subspace spanned by
	    <m>\uvec_1</m>, <m>\uvec_2</m>, and <m>\uvec_3</m>.
	  </p>
	</li>
      </ul>
      Since variance is additive, we have
      <md>
	<mrow>
	  V_{W_1} = \amp V_{\uvec_1} = 13
	</mrow>
	<mrow>
	  V_{W_2} = \amp V_{\uvec_1} + V_{\uvec_2} = 23
	</mrow>
	<mrow>
	  V_{W_3} = \amp V_{\uvec_1} + V_{\uvec_2} + V_{\uvec_3} = 25
	</mrow>
      </md>
    </p>

    <p>
      Notice how we capture more of the total variance as we increase
      the dimension of the subspace onto which the data is projected.
      Eventually, projecting the data onto <m>W_3</m> gives the total
      variance 25.  Since we don't lose any of the variance projecting
      onto <m>W_3</m>, the data must already lie in <m>W_3</m>.
    </p>

    <p>
      Another way to think about this is to notice that the eigenvalue
      associated to the eigenvector <m>\uvec_4</m> is zero; this means
      that the variance of the data projected along the line defined
      by <m>\uvec_4</m> is zero so all the points project to the zero
      vector.  From this, we conclude that the data must lie in the
      orthogonal complement of <m>\uvec_4</m>.
      Similarly, the eigenvalue associated to <m>\uvec_5</m> is zero
      so the data must lie in the orthogonal complement of
      <m>\uvec_5</m> as well.  Since the data lies in the orthogonal
      complements of both <m>\uvec_4</m> and <m>\uvec_5</m>, it must
      lie in <m>W_3</m>, the three-dimensional subspace spanned by
      <m>\uvec_1</m>, 
      <m>\uvec_2</m>, and <m>\uvec_3</m>.
    </p>

    <p>
      Of course, this is a contrived example.  Typically, the presence
      of noise in a dataset means that we do not expect all the points
      to be wholly contained in a smaller dimensional subspace.  The
      two-dimensional subspace <m>W_2</m> captures 23/25 = 92% of the
      variance.  Depending on the situation, we may want to write off
      the remaining 8% of the variance as noise in exchange for the
      convenience of working with a smaller dimensional subspace.
    </p>

    <p>
      The eigenvectors <m>\uvec_j</m> are called <em>principal
      components</em>, and we order them so their associated
      eigenvalues decrease.  Generally speaking, we hope that the
      first few principal components capture most of the variance.  In
      any case, adding more principal components captures
      more of the variance but at the expense of working with a higher
      dimensional subspace.  As we'll see later, we will seek a
      balance using a number of principal components large enough to
      capture most of the variance but small enough to be easy to work
      with. 
    </p>

    <activity>
      <statement>
	<p>
	  We will work here with a dataset having 100 3-dimensional
	  demeaned data points.  Evaluating the following cell will
	  plot those data points and define the demeaned data matrix
	  <c>A</c> whose shape is <m>3\times100</m>.
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/pca-demo.py', globals())
	    </input>
	  </sage>
	  Notice that the data appears to cluster around a plane
	  though it does not seem to be wholly contained within that
	  plane.

	  <ol label="a.">
	    <li>
	      <p>
		Use the matrix <c>A</c> to construct the covariance
		matrix <m>C</m>.  Then determine the variance in the
		direction of 
		<m>\uvec=\threevec{1/3}{2/3}{2/3}</m>?
		<sage>
		  <input>
<!--
C = 1/100*A*A.T
D, P = C.right_eigenmatrix()		    
u1 = P.column(0)
u2 = P.column(2)
Q = matrix([u1,u2]).T
-->
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the eigenvalues of <m>C</m> and determine the
		total variance.
		<sage>
		  <input>
		  </input>
		</sage>
		Notice that Sage does not necessarily sort the
		eigenvalues in decreasing order.
	      </p>
	    </li>

	    <li>
	      <p>
		Use the <c>right_eigenmatrix()</c> command to find the
		eigenvectors of <m>C</m>.  Then remember that the Sage
		command <c>B.column(1)</c> retrieves the vector
		represented by the second column of <c>B</c> and
		define vectors <c>u1</c>, <c>u2</c>, and <c>u3</c>
		representing the three principal components in order of
		decreasing eigenvalues.
	      </p>
	    </li>

	    <li>
	      <p>
		What fraction of the total variance is captured by
		projecting the data onto <m>W_1</m>, the subspace
		spanned by <m>\uvec_1</m>?  What fraction of the total
		variance is captured by projecting onto <m>W_2</m>,
		the subspace spanned by <m>\uvec_1</m> and
		<m>\uvec_2</m>?  What fraction of the total variance
		do we lose by projecting onto <m>W_2</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		If we project a data point <m>\xvec</m> onto
		<m>W_2</m>, the projection formula tells us we obtain
		<me>
		  \xhat = (\uvec_1\cdot\xvec) \uvec_1 +
		  (\uvec_2\cdot\xvec) \uvec_2.
		</me>
		Rather than viewing the projected data in
		<m>\real^3</m>, we will record the coordinates of
		<m>\xhat</m> in the basis defined by <m>\uvec_1</m>
		and <m>\uvec_2</m>;  that is, we will record the
		coordinates
		<me>
		  \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.  
		</me>
		Construct the matrix <m>Q</m> so that <m>Q^T\xvec =
		\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Since each column of <m>A</m> represents a data point,
		the matrix <m>Q^TA</m> represents the coordinates of
		the projected data points.  Evaluating the following
		cell will plot those projected data points.
		<sage>
		  <input>
		    pca_plot(Q.T*A)
		  </input>
		</sage>
		Notice how this plot enables us to view the data as if
		it were two-dimensional.
		Why is this plot wider than it is tall?
	      </p>
	    </li>
	  </ol>

	</p>
      </statement>
    </activity>

    <p>
      This example is a more realistic illustration of principal component
      analysis.  Starting with the <m>3\times100</m> matrix of
      demeaned data <m>A</m>, we construct the covariance matrix
      <m>C=\frac{1}{100} ~AA^T</m> and study its eigenvalues.  
      Notice that the first two principal components account for more
      than 98% of the variance, which means we can expect the points
      to lie close to <m>W_2</m>, the two-dimensional subspace
      spanned by <m>\uvec_1</m> and <m>\uvec_2</m>.  In fact, the
      plot of the three-dimensional data appears to show that the data
      lies close to a plane.  The analysis we are performing helps us
      to identify that plane quantitatively. 
    </p>

    <p>
      Since <m>W_2</m> is a subspace of <m>\real^3</m>, 
      projecting the data points onto <m>W_2</m> gives a list of 100
      points in <m>\real^3</m>.  In order to visualize them more
      easily, we instead consider the coordinates of the projections
      in the basis defined by <m>\uvec_1</m> and <m>\uvec_2</m>.  For
      instance, we know that the projection of a data point
      <m>\xvec</m> is
      <me>
	\xhat = (\uvec_1\cdot\xvec)\uvec_1 +
	(\uvec_2\cdot\xvec)\uvec_2,
      </me>
      which is a three-dimensional vector.  Instead, we can record the
      coordinates <m>\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>
      and plot them in the two-dimensional coordinate plane, as
      illustrated in <xref ref="fig-pca-coords" />.
    </p>

    <figure xml:id="fig-pca-coords">
      <sidebyside widths="50% 40%">
	<image source = "images/pca-proj" />
	<image source = "images/pca-coords" />
      </sidebyside>
      <caption>
	<p>
	  The projection <m>\xhat</m> of a data point <m>\xvec</m>
	  onto <m>W_2</m> is a three-dimensional vector, which may be
	  represented by the two coordinates describing this vector as
	  a linear combination of <m>\uvec_1</m> and <m>\uvec_2</m>.
	</p>
      </caption>
    </figure>

    <p>
      If we form the matrix <m>Q=\begin{bmatrix}\uvec_1 \amp \uvec_2
      \end{bmatrix}</m>, then we have
      <me>
	Q^T\xvec = \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.
      </me>
      This means that the columns of <m>Q^TA</m> represent the
      coordinates of the projected points, which may now be plotted in
      the plane.
    </p>

    <p>
      In this plot, the first coordinate, represented by the
      horizontal coordinate, represents the projection of a data point
      onto the line defined by <m>\uvec_1</m> while the second
      coordinate represents the projection onto the line defined by
      <m>\uvec_2</m>.  Since <m>\uvec_1</m> is the first principal
      component, the variance in the <m>\uvec_1</m> direction is
      greater than the variance in the <m>\uvec_2</m> direction.  For
      this reason, the plot will be more spread out in the horizontal
      direction than in the vertical.
    </p>

  </subsection>

  <subsection>
    <title> Using Principal Component Analysis </title>

    <p>
      Now that we've explored the ideas behind principal component
      analysis, we will look at a few examples that illustrate its use.
    </p>

    <activity>
      <statement>
	<p>
	  The next cell will load a dataset describing the dietary
	  habits of citizens in each of the four nations of the United
	  Kingdom.  The units for each entry are grams per person per
	  week.  
	  <sage>
	    <input>
import pandas as pd
df = pd.read_csv('http://merganser.math.gvsu.edu/david/linear.algebra/ula/data/uk-diet.csv', index_col=0)
data_mean = vector(df.T.mean())
A = matrix([vector(row) for row in (df.T-df.T.mean()).values]).T
df

	    </input>
	  </sage>
	  We will view this as a dataset having four points in
	  <m>\real^{17}</m>.  As such, it is impossible to visualize
	  and studying the numbers themselves doesn't lead to much
	  insight.
	</p>

	<p>
	  In addition to loading the data, evaluating the cell above
	  created a vector <c>data_mean</c>, which is the mean of the
	  four data points, and <c>A</c>, the <m>17\times4</m> matrix
	  of demeaned data.
	  <ol label="a.">
	    <li>
	      <p>
		What is the average consumption of Beverages across
		the four nations?
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>C</m> and its
		eigenvalues.  Because there are four points in
		<m>\real^{17}</m> whose mean is zero, there are only
		three nonzero eigenvalues.  State their values.
	      </p>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance does the
		first principal component account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the first principal component <m>\uvec_1</m> and
		project the four demeaned data points onto the line
		defined by <m>\uvec_1</m>.  Plot those points on <xref
		ref="fig-pca-1d" />
	      </p>

	      <figure xml:id="fig-pca-1d">
		<sidebyside width="90%">
		  <image source = "images/pca-plot-1" />
		</sidebyside>
		<caption>
		  <p>
		    A plot of the demeaned data projected onto the
		    first principal component.
		  </p>
		</caption>
	      </figure>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance do the first
		two principal components account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the cordinates of the demeaned data points
		projected onto <m>W_2</m>, the two-dimensional
		subspace of <m>\real^{17}</m> spanned by
		the first two principal components.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>

	      <p>
		Plot these coordinates in <xref ref="fig-pca-2d" />.
	      </p>

	      <figure xml:id="fig-pca-2d">
		<sidebyside width="90%">
		  <image source="images/pca-plot-2" />
		</sidebyside>
		<caption>
		  <p>
		    The coordinates of the demeaned data points
		    projected onto the first two principal components.
		  </p>
		</caption>
	      </figure>
	    </li>

	    <li>
	      <p>
		What information do these plots reveal that is not
		clear from consideration of the original data points?
	      </p>
	    </li>

	    <li>
	      <p>
		Study the first principal component <m>\uvec_1</m>
		and find the first component of <m>\uvec_1</m>, which
		corresponds to the dietary category Alcoholic Drinks.
		(To do this, you may wish to use <c>N(u1,
		digits=2)</c> for a result that's easier to read.)
		If a data point lies on the far right side of the plot
		in <xref ref="fig-pca-2d" />, what does it mean about
		that nation's consumption of Alcoholic Drinks?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

    <p>
      This activity demonstrates how principal component analysis
      enables us to extract information from a dataset that may not be
      readily apparent otherwise.  As in our previous example, we see
      that the data points lie, on average, quite close to a
      two-dimensional subspace of <m>\real^{17}</m>.  In fact,
      <m>W_2</m>, the subspace spanned by the first two principal
      components, accounts for more than 96% of the variance.
      More importantly, when we project the data onto <m>W_2</m>, it
      becomes apparent that Northern Ireland is fundamentally
      different from the other three nations.
    </p>

    <p>
      With some additional thought, we can determine more specific
      ways in which Northern Ireland is different.  On the
      two-dimensional 
      plot, Northern Ireland lies far to the right compared to the
      other three nations.  Since the data has been demeaned, the
      origin <m>(0,0)</m> in this plot corresponds to the average of
      the four nations.  The coordinates of the point representing
      Northern Ireland are about <m>(477, 59)</m>, meaning that the
      projected data point differs from the mean by about
      <m>477\uvec_1+59\uvec_2</m>.
    </p>

    <p>
      Let's just focus on the contribution from <m>\uvec_1</m>.  We
      see that the ninth component of <m>\uvec_1</m>, the one that
      describes Fresh Fruit, is about -0.63.  This means that
      <m>477\uvec_1</m> differs from the mean by about <m>477(-0.63) =
      -300</m> grams per person per week.  So roughly speaking, people
      in Northern Ireland are eating about 300 fewer grams of
      Fresh Fruit than the average across the three countries.  This is
      borne out by looking at the origin data, which shows that the
      consumption of Fresh Fruit in Northern Ireland is significantly
      less than the other nations.  Examing the other components of
      <m>\uvec_1</m> shows other categories in which Northern Ireland
      is significantly different.
    </p>

    <activity>
      <statement>
	<p>
	  In this activity, we're going to look at a
	  <url href="https://archive.ics.uci.edu/ml/datasets/Iris">
	  well-known dataset 
	  </url>
	  that records four measurements of each of 150 irises.
	  There are three species of irises, each of which
	  appears 50 times in the data set.  Examples of the three
	  species are shown in <xref ref="fig-iris" />.
	  For each flower, the length and width of its sepal and the
	  length and width of its petal, all in centimeters, are
	  recorded. 
	</p>

	<figure xml:id="fig-iris">
	  <sidebyside width="90%">
	    <image
		source="https://thegoodpython.com/assets/images/iris-species.png"/>
	  </sidebyside>
	  <caption>
	    <p>
	      The three species of irises represented in the data set:
	      iris setosa, iris
	      versicolor, and iris virginica.  Photo credit:  <url
	      href="https://thegoodpython.com/iris-dataset/"> The Good
	      Python </url>
	    </p>
	  </caption>
	</figure>

	<p>
	  Evaluating the following cell will load the data set, which
	  consists of 150 points in <m>\real^4</m>.  In addition,
	  we have a vector <c>data_mean</c>, a four-dimensional
	  vector holding the mean of the data points, and <c>A</c>,
	  the <m>4\times150</m> demeaned data matrix.
	  <sage>
	    <input>
sage.repl.load.load('http://merganser.math.gvsu.edu/david/linear.algebra/ula/python/pca-iris.py', globals())
df


	    </input>
	  </sage>

	  Since the data is four-dimensional, we are not able to
	  visualize it.  Of course, we could forget about two of the
	  measurements and plot the 150 points represented by their,
	  say,  
	  sepal length and sepal width.

	  <sage>
	    <input>
sepal_plot()	      
	    </input>
	  </sage>
	  <ol label="a.">
	    <li>
	      <p>
		What is the mean sepal width?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>C</m> and its
		eigenvalues.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the fraction of variance for which the first two
		principal components account?
	      </p>
	    </li>

	    <li>
	      <p>
		Construct the first two principal components
		<m>\uvec_1</m> and <m>\uvec_2</m> along with the
		matrix <m>Q</m> whose columns are <m>\uvec_1</m> and
		<m>\uvec_2</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		As we have seen, the columns of the matrix <m>Q^TA</m>
		hold the coordinates of the demeaned data points after
		projecting onto <m>W_2</m>, the subspace spanned by
		the first two principal components.  Evaluating the
		following cell shows a plot of these coordinates.
		<sage>
		  <input>
pca_plot(Q.T*A)		    
		  </input>
		</sage>

		Suppose we have a flower whose coordinates in this
		plane are <m>(-2.5, -0.75)</m>.  To what species does
		this iris most likely belong?  Find an estimate of the
		sepal length, sepal width, petal length, and petal
		width for this flower.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose you have an iris, but you only know that its
		sepal length is 5.65 cm and its sepal width is 2.75
		cm.  Knowing only these two measurements, determine
		the coordinates <m>(c_1, c_2)</m> in the plane where
		this iris lies.  To what species does this iris most
		likely belong?  Now estimate the petal length and petal
		width of this iris.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose you find another iris whose sepal width is 3.2
		cm and whose petal width is 2.2 cm.  Find the
		coordinates <m>(c_1, c_2)</m> of this iris and
		determine the species to which it most likely
		belongs.  Also, estimate the sepal length and the
		petal length.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	  </ol>

      
	</p>
      </statement>
    </activity>
      

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section has explored principal component analysis as a
      technique to reduce the dimension of a dataset.  From the
      demeaned data matrix <m>A</m>, we form the covariance matrix
      <m>C= \frac1N ~AA^T</m>, where <m>N</m> is the number of data
      points. 
      <ul>
	<li>
	  <p>
	    The eigenvectors <m>\uvec_1, \uvec_2, \ldots \uvec_m</m>,
	    of <m>C</m> are called the principal components.  We arrange
	    them so that their corresponding eigenvalues are in
	    decreasing order.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>W_n</m> is the subspace spanned by the first
	    <m>n</m> principal components, then the variance of the
	    demeaned data projected onto <m>W_n</m> is the sum of the
	    first <m>n</m> eigenvalues.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>Q</m> is the matrix whose columns are the first
	    <m>n</m> principal components, then the columns of
	    <m>Q^TA</m> hold the coordinates, expressed in the basis
	    <m>\uvec_1,\ldots,\uvec_n</m>, of the data once
	    projected onto <m>W_n</m>.
	  </p>
	</li>

	<li>
	  <p>
	    We hope to use a number of principal components that is
	    large enough to capture most
	    of the variance in the dataset but small enough to be
	    manageable.
	  </p>
	</li>
      </ul>
    </p>
	    
	    

  </subsection>
  
</section>
