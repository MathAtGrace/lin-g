<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-svd-intro"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> The Singular Value Decomposition </title>

  <introduction>
    <p>
      The power of the Spectral Theorem has animated the past few
      sections.  In particular, we used the fact that symmetric
      matrices can be orthogonally diagonalized to simplify quadratic
      forms, which enabled us to use principal component analysis to
      reduce the dimension of a dataset.  More specifically, the fact
      that the covariance matrix <m>C</m> is symmetric means that we
      can write <m>C=QDQ^T</m>, where the columns of <m>Q</m> provide
      the principal components and the diagonal entries of <m>D</m>
      indicate the relative importance of each of those components.
    </p>

    <p>
      But what about matrices that are not symmetric or even square?
      For instance, the following matrices are not diagonalizable,
      much less orthogonally so:
      <me>
	\begin{bmatrix}
	2 \amp 1 \\
	0 \amp 2
	\end{bmatrix},
	\hspace{24pt}
	\begin{bmatrix}
	1 \amp 1 \amp 0 \\
	-1 \amp 0 \amp 1
	\end{bmatrix}.
      </me>
      In this section, we will begin exploring a description of
      matrices called the <em>singular value decomposition</em>.  Most
      notably, we will see that <em>every</em> matrix has a singular
      value decomposition, and that this decomposition is, in several
      important ways, analogous to the orthogonal diagonalization of a
      symmetric matrix.  Rather than writing <m>QDQ^T</m>, we will be
      able to express any matrix as a 
      product <m>A=U\Sigma V^T</m>, where <m>U</m> and <m>V</m> are
      orthogonal matrices whose columns are called <em>singular
      vectors</em>.  The entries of the diagonal matrix <m>\Sigma</m>
      will indicate the relative importance of the singular vectors.
    </p>

    <exploration>
      <statement>
	<p>
	  Before we get started, it will be helpful to remember some
	  concepts from earlier parts of the course.
	  <ol label="a.">
	    <li>
	      <p>
		What do we mean by the rank of a matrix <m>A</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		What do we mean by <m>\nul(A)</m>, the null space of a
		matrix <m>A</m> and how is its dimension expressed in
		terms of <m>\rank(A)</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		What do we mean by <m>\col(A)</m>, the column space of
		a matrix 
		<m>A</m> and how is its dimension expressed in terms
		of <m>\rank(A)</m>?  What is the relationship between
		<m>\col(A)</m> and questions about the
		consistency of an equation <m>A\xvec = \bvec</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		How does the matrix <m>A^T</m> help us understand the
		orthogonal complement of <m>\col(A)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the orthogonal diagonalization of the symmetric
		matrix <m>A = \begin{bmatrix} 2 \amp 1 \\ 1 \amp 2
		\end{bmatrix}</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	    <li>
	      <p>
		What is the maximum value of the quadratic form
		<m>q_A(\uvec)</m> where <m>\uvec</m> is a unit vector
		and in what direction does this maximum occur?
	      </p>
	    </li>
	    
	  </ol>
	</p>

      </statement>
    </exploration>
      
  </introduction>

  <subsection>
    <title> An introduction to singular value decompositions </title>

    <p>
      We will begin by explaining what a singular value decomposition
      is and how it generalizes an orthogonal diagonalization.  First,
      recall how the orthogonal diagonalization of a symmetric matrix
      is formed: if <m>A</m> is symmetric, we write <m>A = QDQ^T</m>
      where the diagonal entries of <m>D</m> are the eigenvalues of
      <m>A</m> and the columns of <m>Q</m> are the associated
      eigenvectors.
    </p>

    <p>
      Now imagine an alternate world where we didn't know anything
      about eigenvalues and eigenvectors.  If we have a symmetric
      matrix <m>A</m>, we could construct its quadratic form
      <m>q_A</m> and study the maximum and minimum values
      <m>q_A(\uvec)</m> among all unit vectors <m>\uvec</m>.  In this
      way, we would find the eigenvalues of <m>A</m> as the
      maximum and minimum values and the associated eigenvectors as
      the directions in which the extrema occur.  That is, the
      quadratic form would lead us to discover the eigenvalues and
      eigenvectors of a symmetric matrix.
    </p>

    <p>
      A general matrix, particularly a matrix that is not square, may
      not have eigenvalues and eigenvectors, but we can discover
      analogous features, called <em>singular values</em> and
      <em>singular vectors</em>, by studying a function somewhat
      similar to a quadratic form.  More specifically, any matrix
      <m>A</m> defines a function
      <me>
	l_A(\xvec) = \len{A\xvec},
      </me>
      that measures the length of <m>A\xvec</m>.  This
      function is not a quadratic form, but we can discover the
      singular values and vectors by looking for the maximum and
      minimum of this function <m>l_A(\uvec)</m> among all unit vectors
      <m>\uvec</m>.
    </p>
    
    <p>
      The next activity demonstrates how we can find
      singular values and vectors geometrically.
    </p>

    <activity>
      <statement>
	<p>
	  The following interactive figure will help us explore
	  singular values and vectors geometrically before we begin a
	  more algebraic approach.  This figure is also available at
	  <url href="http://gvsu.edu/s/0YE"> gvsu.edu/s/0YE </url>.
	</p>

	<figure xml:id="js-svd">
	  <caption>
	    Singular values, right singular vectors and left singular
	    vectors 
	  </caption>  
	  
	  <interactive xml:id="interactive-svd"
		       platform="javascript" width="100%"
		       aspect="15:10"
		       source = "interactives/figures.js
				 interactives/singular.js">
	    <sbsgroup>
	      <sidebyside width="90%">
		<slate xml:id="bottomsliders" aspect="7:1"
		       surface="canvas" />
	      </sidebyside>
	      <sidebyside widths="48% 48%">
		<slate xml:id="bottomleft" aspect="1:1" surface="canvas" />
		<slate xml:id="bottomright" aspect="1:1" surface="canvas" />
	      </sidebyside>
	    </sbsgroup>
	    <instructions>
	      <p>
		The four sliders at the top of this figure enable us
		to choose a <m>2\times2</m> matrix <m>A</m>.  Below on
		the left, we see the unit circle and the red unit
		vector <m>\xvec</m>, which may be varied by clicking
		in the head of the vector and dragging it to a new
		unit vector.
	      </p>
	    </instructions>
	  </interactive>
	</figure>

	<p>
	  <ol label="a.">
	    <li>
	      <p>
		Let's begin by studying the matrix
		<m>A=\begin{bmatrix} 1 \amp 2 \\ 2 \amp 1
		\end{bmatrix}
		</m>.  Of course, this is a symmetric matrix that we
		know can be written as <m>A=QDQ^T</m> where
		<me>
		  D = \begin{bmatrix}
		  3 \amp 0 \\
		  0 \amp -1
		  \end{bmatrix},\hspace{24pt}
		  Q = \begin{bmatrix}
		  1/\sqrt{2} \amp -1/\sqrt{2} \\
		  1/\sqrt{2} \amp 1/\sqrt{2} \\
		  \end{bmatrix}.
		</me>
	      </p>

	      <p>
		On the right side of the figure, you will see the
		vector <m>A\xvec</m> in gray.  The height of the blue
		rectangle is <m>l_A(\xvec)=\len{A\xvec}</m>, the length of
		<m>A\xvec</m>.  Move the vector <m>\xvec</m> so that
		<m>l_A(\xvec)</m> is as large as possible.

		<ol label="i.">
		  <li>
		    <p>
		      The
		      maximum value of <m>l_A(\xvec)</m> is called the first
		      singular value and denoted <m>\sigma_1</m>.  What is
		      the value of <m>\sigma_1</m>?
		    </p>
		  </li>

		  <li>
		    <p>
		      The vector <m>\xvec</m> that gives the maximum value
		      of <m>l_A(\xvec)</m> is <m>\vvec_1</m>, the first
		      <em>right singular vector</em>.  This may seem a
		      little confusing because the right singular vector
		      appears on the left side of the diagram, but the
		      reason for this terminology will be clear later.
		      What do you find for
		      <m>\vvec_1</m>?
		    </p>
		  </li>

		  <li>
		    <p>
		      Finally, <em>left singular vector</em> <m>\uvec_1</m>
		      is the unit vector, shown on the right in blue, that
		      is parallel to <m>A\vvec_1</m>.  Once again, it may
		      not be clear why the left singular vector appears on
		      the right of the diagram, but we'll explain this
		      later.  What do you find for the left singular vector?
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Now move <m>\xvec</m> so that <m>l_A(\xvec)</m> is as
		small as possible.  What do you find for
		<ol label="i.">
		  <li>
		    <p>
		      the second singular
		      value <m>\sigma_2</m>, the smallest value of
		      <m>l_A(\xvec)</m>?
		    </p>
		  </li>

		  <li>
		    <p>
		      the right singular vector <m>\vvec_2</m>, the
		      vector <m>\xvec</m> at which the minimum occurs?
		    </p>
		  </li>

		  <li>
		    <p>
		      the left singular vector <m>\uvec_2</m>, the
		      unit vector parallel to <m>A\vvec_2</m>?
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		For this symmetric matrix <m>A</m>, how do the
		singular values and singular vectors appear to be
		related to the eigenvalues and eigenvectors of
		<m>A</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Let's now look at the matrix
		<m>
		  A = \begin{bmatrix}
		  1 \amp 2 \\
		  -1 \amp 2
		  \end{bmatrix}
		</m>.
		Move the vector <m>\xvec</m> so that <m>l_A(\xvec)</m>
		is as large as possible.  What do you find for the
		singular value <m>\sigma_1</m>, the right singular
		vector <m>\vvec_1</m> at which this maximum occurs,
		and the left singular vector <m>\uvec_1</m>, the unit
		vector that is parallel to <m>A\vvec_1</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Move the vector <m>\xvec</m> again so that
		<m>l_A(\xvec)</m> is as small as possible.  What do
		you find for the second singular value <m>\sigma_2</m>,
		right singular vector <m>\vvec_2</m>, and left
		singular vector <m>\uvec_2</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		What do you notice about the angle between the two
		right singular vectors <m>\vvec_1</m> and
		<m>\vvec_2</m>?  What do you notice about the angle
		between the two left singular vectors <m>\uvec_1</m>
		and <m>\uvec_2</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Explain why <m>A\vvec_1 = \sigma_1\uvec_1</m> and
		<m>A\vvec_2 = \sigma_2\uvec_2</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
    </activity>

    <p>
      Before moving on, let's summarize these results and take a look
      ahead.
    </p>

    <example>
      <statement>
	<p>
	  When
	  <m>A=\begin{bmatrix}
	  1 \amp 2 \\
	  2 \amp 1
	  \end{bmatrix}</m>,
	  we find singular values and vectors
	  <md>
	    <mrow>
	      \sigma_1 = 3, \hspace{24pt}\amp \vvec_1 =
	      \twovec{1/\sqrt{2}}{1/\sqrt{2}}, 
	      \amp \uvec_1 = \twovec{1/\sqrt{2}}{1/\sqrt{2}}
	    </mrow>
	    <mrow>
	      \sigma_2 = 1, \hspace{24pt}\amp \vvec_2 =
	      \twovec{-1/\sqrt{2}}{1/\sqrt{2}}, 
	      \amp \uvec_2 = \twovec{1/\sqrt{2}}{-1/\sqrt{2}}
	    </mrow>
	  </md>
	  Notice that the singular values <m>\sigma_j</m> 
	  are the absolute values of the eigenvalues of
	  <m>A</m> 
	  and that the right singular vectors <m>\vvec_j</m> are
	  eigenvectors of <m>A</m>. The left singular vectors are also
	  eigenvectors and satisfy <m>\uvec_j = \pm\vvec_j</m> depending
	  on whether the associated eigenvalue is positive or negative.
	</p>
	
	<p>
	  Just as the orthogonal diagonalization leads to a factorization
	  <m>A=QDQ^T</m>, the singular values and vectors lead to a
	  factorization as well.  We will explain this more fully later,
	  but for now, let's get a sense of where we're headed.  We form
	  three matrices:  <m>U</m> having columns <m>\uvec_1</m> and
	  <m>\uvec_2</m>, <m>V</m> having columns <m>\vvec_1</m> and
	  <m>\vvec_2</m>, and <m>\Sigma</m>, a diagonal matrix whose
	  entries are the singular values:
	  <me>
	    U = \begin{bmatrix}
	    1/\sqrt{2} \amp 1/\sqrt{2} \\
	    1/\sqrt{2} \amp -1/\sqrt{2}
	    \end{bmatrix},
	    \Sigma = \begin{bmatrix}
	    3 \amp 0 \\
	    0 \amp 1
	    \end{bmatrix},
	    V = \begin{bmatrix}
	    1/\sqrt{2} \amp -1/\sqrt{2} \\
	    1/\sqrt{2} \amp 1/\sqrt{2}
	    \end{bmatrix}.
	  </me>
	  It turns out that the product <m>U\Sigma V^T</m> forms the original
	  matrix <m>A</m>;  that is,
	  <me>
	    A = \begin{bmatrix}
	    1 \amp 2 \\
	    2 \amp 1
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    1/\sqrt{2} \amp 1/\sqrt{2} \\
	    1/\sqrt{2} \amp -1/\sqrt{2}
	    \end{bmatrix}
	    \begin{bmatrix}
	    3 \amp 0 \\
	    0 \amp 1
	    \end{bmatrix}
	    \begin{bmatrix}
	    1/\sqrt{2} \amp -1/\sqrt{2} \\
	    1/\sqrt{2} \amp 1/\sqrt{2}
	    \end{bmatrix}.
	  </me>
	  We call this factorization <m>A=U\Sigma V^T</m>, the
	  <em>singular value decomposition</em> of <m>A</m> and note
	  its similarity to the orthogonal diagonalization
	  <m>A=QDQ^T</m>. 
	</p>
      </statement>
    </example>

    <example>
      <statement>
	<p>
	  Let's now look at the matrix
	  <m>A=\begin{bmatrix}
	  1 \amp 2 \\
	  -1 \amp 2
	  \end{bmatrix}
	  </m>.  The eigenvalues of this matrix are not real so
	  <m>A</m> is
	  not diagonalizable, much less orthogonally diagonalizable.
	  However, the previous activity shows that we have singular
	  values and vectors:
	  <md>
	    <mrow>
	      \sigma_1 = \sqrt{8}, \hspace{24pt}\amp \vvec_1 =
	      \twovec01
	      \amp \uvec_1 = \twovec{1/\sqrt{2}}{1/\sqrt{2}}
	    </mrow>
	    <mrow>
	      \sigma_2 = \sqrt{2}, \hspace{24pt}\amp \vvec_2 =
	      \twovec10
	      \amp \uvec_2 = \twovec{1/\sqrt{2}}{-1/\sqrt{2}}
	    </mrow>
	  </md>
	</p>

	<p>
	  Notice that there is a connection between a singular value
	  and its associated singular vectors.  Because
	  <me>
	    \sigma_j = l_A(\vvec_j) = \len{A\vvec_j},
	  </me>
	  we see that <m>\sigma_j</m> equals the length of
	  <m>A\vvec_j</m>.  Since <m>\uvec_j</m> is a unit vector
	  whose direction is the same as <m>A\vvec_j</m>, we have
	  <me>
	    A\vvec_j = \sigma_j\uvec_j,
	  </me>
	  a key fact we'll soon use when we find singular value
	  decompositions algebraically.
	</p>

	<p>
	  Once again, we construct the matrices <m>U</m>, <m>V</m>,
	  and <m>\Sigma</m> as before,
	  <me>
	    U = \begin{bmatrix}
	    1/\sqrt{2} \amp 1/\sqrt{2} \\
	    1/\sqrt{2} \amp -1/\sqrt{2}
	    \end{bmatrix},
	    \Sigma = \begin{bmatrix}
	    \sqrt{8} \amp 0 \\
	    0 \amp \sqrt{2}
	    \end{bmatrix},
	    V = \begin{bmatrix}
	    0 \amp 1 \\
	    1 \amp 0
	    \end{bmatrix}.
	  </me>
	  and see that <m>A=U\Sigma V^T</m>:
	  <me>
	    A = \begin{bmatrix}
	    1 \amp 2 \\
	    -1 \amp 2
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    1/\sqrt{2} \amp 1/\sqrt{2} \\
	    1/\sqrt{2} \amp -1/\sqrt{2}
	    \end{bmatrix}
	    \begin{bmatrix}
	    \sqrt{8} \amp 0 \\
	    0 \amp \sqrt{2}
	    \end{bmatrix}
	    \begin{bmatrix}
	    0 \amp 1 \\
	    1 \amp 0
	    \end{bmatrix}.
	  </me>
	  Notice that the right singular vectors <m>\vvec_1</m> and
	  <m>\vvec_2</m> are orthgonal to one another and that the
	  left singular vectors <m>\uvec_1</m> and <m>\uvec_2</m> are
	  orthogonal.  We will soon verify that the orthogonality of
	  the right and left singular vectors is a general principle.
	  For now, however, notice that this means that <m>U</m> and
	  <m>V</m> are orthogonal matrices as their columns form 
	  orthonormal bases of <m>\real^2</m>.
	</p>

	<p>
	  To summarize, we see that,
	  even though <m>A</m> is not diagonalizable, it still has
	  a singular value decomposition <m>A=U\Sigma V^T</m> where
	  <m>\Sigma</m> is diagonal and <m>U</m> and <m>V</m> are
	  orthogonal.  In 
	  fact, we will see that <em>every</em> matrix, even those
	  that are not square, has a singular value decomposition,
	  and we can apply these decompositions to study general
	  matrices in the same way we used orthogonal diagonalizations
	  to study symmetric matrices.
	</p>
      </statement>
    </example>
  </subsection>

    <subsection>
      <title> Finding singular value decompositions algebraically
      </title>

      <p>
	Now that we have a sense of what singular value
	decompositions are, we will explore an algebraic technique for
	finding them.  We earlier found singular values by studying the
	function <m>l_A(\xvec) = \len{A\xvec}</m>.  Our key
	observation is that, while <m>l_A</m> is not a quadratic form,
	its square is.  That is, we will define
	<me>
	  q(\xvec) =
	  \left(l_A(\xvec)\right)^2 = \len{A\xvec}^2
	</me>
	and notice that
	<me>
	  q(\xvec) = \len{A\xvec}^2 = (A\xvec)\cdot(A\xvec) =
	  \xvec\cdot(A^TA\xvec).
	</me>
	To find singular values and vectors, we sought the maximum and
	minimum values of <m>l_A(\xvec) = \sqrt{q(\xvec)}</m>.
	Because <m>q(\xvec)</m> is a quadratic form, we know how to
	find its maximum and minimum values using an orthogonal
	diagonalization of its associated matrix <m>A^TA</m>.  The matrix
	<m>A^TA</m> is called the <em>Gram matrix</em> of <m>A</m>, and
	the next activity explores its role in constructing the
	singular value decomposition of <m>A</m>.
      </p>

      <activity>
	<statement>
	  <p>
	    In this activity, we will construct the singular value
	    decomposition of
	    <m>
	      A=\begin{bmatrix} 1 \amp 0 \amp -1 \\
	      1 \amp 1 \amp 1
	      \end{bmatrix}
	    </m>.  Notice that this matrix is not square so there are
	    no eigenvalues and eigenvectors associated to it.
	    <ol label="a.">
	      <li>
		<p>
		  Construct the Gram matrix <m>G=A^TA</m> and explain
		  why it is a symmetric matrix.
		  <sage>
		    <input>

		    </input>
		  </sage>
		</p>
	      </li>

	      <li>
		<p>
		  Find an orthogonal diagonalization of <m>G</m>; that
		  is, find matrices <m>Q</m> and <m>D</m> so that <m>G
		  = QDQ^T</m>.
		</p>
	      </li>

	      <li>
		<p>
		  The eigenvalues of <m>G</m> give the critical
		  values of the quadratic form
		  <me>
		    Q_G(\xvec) = \xvec\cdot(G\xvec) = \len{A\xvec}^2
		    = \left(l_A(\xvec)\right)^2.
		  </me>
		  Explain why the singular values of <m>A</m> are the
		  square roots of the eigenvalues of the Gram matrix
		  and state the singular values of <m>A</m> taking
		  care of arrange them in decreasing order.
		</p>
	      </li>

	      <li>
		<p>
		  Explain why the eigenvectors of <m>G</m> define the
		  critical directions of <m>l_A(\xvec)</m> so that the 
		  right singular vectors of <m>A</m> are eigenvectors
		  of <m>G</m>.  State the right singular vectors
		  <m>\vvec_1</m>, <m>\vvec_2</m>, and <m>\vvec_3</m>.
		</p>
	      </li>

	      <li>
		<p>
		  Explain why this observation tells us that the right
		  singular vectors <m>\vvec_j</m> form an orthonormal
		  basis for <m>\real^3</m>.
		</p>
	      </li>

	      <li>
		<p>
		  Remembering that <m>A\vvec_j = \sigma_j\uvec_j</m>,
		  find the left singular vectors <m>\uvec_1</m> and
		  <m>\uvec_2</m>.  Notice that we can only create two
		  left singular vectors in this way since there are
		  only two nonzero singular values.
		  <sage>
		    <input>

		    </input>
		  </sage>
		</p>
	      </li>

	      <li>
		<p>
		  We can now explain why the left singular vectors are
		  orthogonal.  To do so, first use the fact that
		  <me>
		    (A\vvec_1)\cdot(A\vvec_2) = \vvec_1
		    \cdot(A^TA\vvec_2)
		  </me>
		  to explain why <m>A\vvec_1</m> and <m>A\vvec_2</m>
		  are orthogonal to one another.  Then use the fact
		  that <m>A\vvec_j = \sigma_j\uvec_j</m> to explain
		  why <m>\uvec_1</m> and <m>\uvec_2</m> are
		  orthogonal.
		</p>
	      </li>

	      <li>
		<p>
		  As before, form the matrices <m>U</m> and <m>V</m>
		  from the left and right singular vectors.  Now form
		  <m>\Sigma</m> so that it has the same shape as
		  <m>A</m>:
		  <me>\Sigma = \begin{bmatrix}
		  \sigma_1 \amp 0 \amp 0 \\
		  0 \amp \sigma_2 \amp 0
		  \end{bmatrix}
		  </me>
		  and verify that <m>A = U\Sigma V^T</m>.
		  <sage>
		    <input>
		    </input>
		  </sage>
		</p>
	      </li>

	    </ol>
	  </p>
	</statement>
      </activity>

      <p>
	In this activity, we found the singular value decomposition of
	a <m>2\times3</m> matrix <m>A</m> using its Gram matrix
	<m>G</m> to
	relate the 
	function <m>l_A(\xvec)</m> to the quadratic form
	<m>q_G(\xvec)</m>;  in particular, we keep in mind that
	<m>l_A(\xvec) = \sqrt{q_G(\xvec)}</m>.
      </p>

      <p>
	Constructing the Gram matrix of
	<m>A=\begin{bmatrix}
	1 \amp 0 \amp -1 \\
	1 \amp 1 \amp 1
	\end{bmatrix}
	</m> gives the <m>3\times3</m> matrix
	<me>
	  G = \begin{bmatrix}
	  2 \amp 1 \amp 0 \\
	  1 \amp 1 \amp 1 \\
	  0 \amp 1 \amp 2
	  \end{bmatrix}.
	</me>
	Notice that the Gram matrix is always a symmetric matrix
	because <m>G^T=(A^TA)^T = A^TA=G</m>.  The Spectral Theorem
	then tells us that <m>G</m> is orthogonally diagonalizable,
	and we find that <m>G=QDQ^T</m> where
	<me>
	  D = \begin{bmatrix}
	  3 \amp 0 \amp 0 \\
	  0 \amp 2 \amp 0 \\
	  0 \amp 0 \amp 0
	  \end{bmatrix},\hspace{24pt}
	  Q = \begin{bmatrix}
	  1/\sqrt{3} \amp 1/\sqrt{2} \amp 1/\sqrt{6} \\
	  1/\sqrt{3} \amp 0 \amp -2/\sqrt{6} \\
	  1/\sqrt{3} \amp -1/\sqrt{2} \amp 1/\sqrt{6} 
	  \end{bmatrix}.
	</me>
      </p>

      <p>
	Because <m>l_A(\xvec) = \sqrt{q_G(\xvec)}</m>, the critical
	values of <m>l_A(\xvec)</m> are the square roots of the
	critical values of <m>q_G(\xvec)</m>, which are the
	eigenvalues <m>\lambda_j</m> of <m>G</m>.  This means that
	<m>\sigma_j=\sqrt{\lambda_j}</m> so we have
	<m>\sigma_1 =\sqrt{3}</m>, <m>\sigma_2 = \sqrt2</m>, and
	<m>\sigma_3 = 0</m>.
      </p>

      <p>
	The critical directions of <m>l_A(\xvec)</m> are the same as
	the critical directions of <m>q_A(\xvec)</m> so we find that
	the right singular vectors <m>\vvec_j</m> are formed by the
	eigenvectors of <m>G</m>, which appear as the
	columns of <m>Q</m>.  This shows that
	<me>
	  \vvec_1 = \threevec{1/\sqrt{3}}{1/\sqrt{3}}{1/\sqrt{3}},
	  \hspace{24pt}
	  \vvec_2 = \threevec{1/\sqrt{2}}{0}{-1/\sqrt{2}},
	  \hspace{24pt}
	  \vvec_3 = \threevec{1/\sqrt{6}}{-2/\sqrt{6}}{1/\sqrt{6}}.
	</me>
	Because these are the eigenvectors of the symmetric matrix
	<m>G</m> associated to distinct eigenvalues, we know that they
	are orthogonal and hence form an orthonormal basis of
	<m>\real^3</m>.  The matrix <m>V=\begin{bmatrix}\vvec_1 \amp
	\vvec_2 \amp \vvec_3\end{bmatrix}</m> is therefore orthogonal.
      </p>

      <p>
	We now form the left singular vectors using the relationship
	<m>A\vvec_j = \sigma_j\uvec_j</m>.  In particular, we have
	<md>
	  <mrow>
	    A\vvec_1 \amp = \twovec{0}{\sqrt{3}} = \sqrt{3} \twovec01
	  </mrow>
	  <mrow>
	    A\vvec_2 \amp = \twovec{\sqrt{2}}0 = \sqrt{2} \twovec10
	  </mrow>
	</md>
	showing that
	<me>
	  \uvec_1 = \twovec01, \hspace{24pt} \uvec_2 = \twovec10.
	</me>
      </p>

      <p>
	Notice that the left singular vectors are orthogonal.  This
	follows because <m>(A\vvec_1)\cdot(A\vvec_2) =
	\vvec_1\cdot(A^TA\vvec_2) = 2\vvec_1\cdot\vvec_2 = 0</m>
	because <m>\vvec_2</m> is an eigenvector of <m>A^TA</m> with
	associated eigenvalue 2 and <m>\vvec_1</m> and <m>\vvec_2</m>
	are already known to be orthogonal. Therefore,
	<me>
	  (A\vvec_1)\cdot(A\vvec_2) =
	  \sqrt3\sqrt2~\uvec_1\cdot\uvec_2 = 0,
	</me>
	which explains why the left singular vectors are orthogonal.
      </p>

      <p>
	To construct the singular value decomposition, we form
	<m>\Sigma</m> so that it has the same shape as <m>A</m> and
	write the singular values on
	the diagonal:
	<me>
	  \Sigma = \begin{bmatrix}
	  \sqrt{3} \amp 0 \amp 0 \\
	  0 \amp \sqrt{2} \amp 0
	  \end{bmatrix}.
	</me>
	If we are to have <m>A=U\Sigma V^T</m>, then <m>U</m> will be
	<m>2\times2</m> and <m>V</m> will be <m>3\times3</m>.  This
	leads to
	<me>
	  U = \begin{bmatrix}
	  0 \amp 1 \\
	  1 \amp 0 \\
	  \end{bmatrix},\hspace{10pt}
	  \Sigma = \begin{bmatrix}
	  \sqrt{3} \amp 0 \amp 0 \\
	  0 \amp \sqrt{2} \amp 0
	  \end{bmatrix}, \hspace{10pt}
	  V = \begin{bmatrix}
	  1/\sqrt{3} \amp 1/\sqrt{2} \amp 1/\sqrt{6} \\
	  1/\sqrt{3} \amp 0 \amp -2/\sqrt{6} \\
	  1/\sqrt{3} \amp -1/\sqrt{2} \amp 1/\sqrt{6} 
	  \end{bmatrix}.
	</me>
      </p>

      <p>
	Suppose now that we write a three-dimensional vector
	<m>\xvec</m> in terms of the basis of right singular vectors:
	<me>
	  \xvec = c_1\vvec_1+c_2\vvec_2+c_3\vvec_3.
	</me>
	We then have 
	<m>\xvec = V\threevec{c_1}{c_2}{c_3}</m>, which says, because
	<m>V</m> is orthogonal, that
	<m>\threevec{c_1}{c_2}{c_3} = V^T\xvec</m>.  This means that
	<md>
	  <mrow>
	    A\xvec \amp = A(c_1\vvec_1 + c_2\vvec_2 + c_3\vvec_3)
	  </mrow>
	  <mrow>
	    \amp = c_1A\vvec_1 + c_2A\vvec_2 + c_3A\vvec_3
	  </mrow>
	  <mrow>
	    \amp = c_1\sigma_1\uvec_1 + c_2\sigma_2\uvec_2
	  </mrow>
	  <mrow>
	    \amp = \begin{bmatrix}\uvec_1 \amp \uvec_2 \end{bmatrix}
	    \twovec{c_1\sigma_1}{c_2\sigma_2}
	  </mrow>
	  <mrow>
	    \amp = U
	    \begin{bmatrix}
	    \sigma_1 \amp 0 \amp 0 \\
	    0 \amp \sigma_2 \amp 0
	    \end{bmatrix}
	    \threevec{c_1}{c_2}{c_3}
	  </mrow>
	  <mrow>
	    \amp = U\Sigma V^T\xvec.
	  </mrow>
	</md>
	In other words, we see that <m>A\xvec = U\Sigma V^T\xvec</m>
	for every vector <m>\xvec</m>, which means that <m>A = U\Sigma
	V^T</m>.
      </p>

      <p>
	We have now found the singular value decomposition of
	<m>A</m>.  To summarize, we
	<ul>
	  <li>
	    <p>
	      found the Gram matrix <m>G=A^TA</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      wrote <m>\lambda_j</m>, the eigenvalues of <m>G</m>, in
	      decreasing order, and noted that the associated
	      eigenvectors <m>\vvec_j</m> formed an orthonormal basis.
	    </p>
	  </li>

	  <li>
	    <p>
	      found the singular values
	      <m>\sigma_j=\sqrt{\lambda_j}</m> and the right singular
	      vectors as the eigenvectors <m>\vvec_j</m> of <m>G</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      found the left singular vectors using the relationship
	      <m>A\vvec_j = \sigma_j\uvec_j</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      constructed <m>\Sigma</m> to have the same shape as
	      <m>A</m> with the singular values running down the
	      diagonal
	      and constructed orthogonal matrix <m>U</m> and <m>V</m>
	      from the left and right singular vectors.
	    </p>
	  </li>
	</ul>
      </p>

      <p>
	As we'll see in the next section, some additional work may be
	needed to construct the left singular vectors <m>\uvec_j</m>
	if more of the singular values are zero, but we won't worry
	about that now.  For the time being, let's record our work in
	the following theorem.
      </p>

      <theorem xml:id="theorem-svd">
	<title> The singular value decomposition </title>
	<statement>
	  An <m>m\times n</m> matrix <m>A</m> may be written
	  as <m>A=U\Sigma V^T</m> where <m>U</m> is an orthogonal
	  <m>m\times m</m> matrix, <m>V</m> is an orthogonal
	  <m>n\times n</m> matrix, and <m>\Sigma</m> is an <m>m\times
	  n</m> matrix with zero entries except for the singular
	  values of <m>A</m> in decreasing order on the diagonal.
	</statement>
      </theorem>

      <p>
	Notice that a singular value decomposition of <m>A</m> gives
	us a singular value decomposition of <m>A^T</m>.  More
	specifically, if <m>A=U\Sigma V^T</m>, then
	<me>
	  A^T = (U\Sigma V^T)^T = V\Sigma^T U^T.
	</me>
	In other words, a singular
	value decomposition of <m>A^T</m> is obtained from a
	decomposition of
	<m>A</m> by replacing <m>\Sigma</m> with <m>\Sigma^T</m> and
	interchanging the roles of the left and right singular
	vectors.
      </p>

      <proposition xml:id="prop-svd-transpose">
	<statement>
	  <p>
	    If <m>A=U\Sigma V^T</m>, then <m>A^T = V\Sigma^T U^T</m>.
	    In other words, <m>A</m> and <m>A^T</m> share the same
	    singular values, and the left singular vectors of <m>A</m> are
	    the right singular vectors of <m>A^T</m> and vice-versa.
	  </p>
	</statement>
      </proposition>

      <p>
	As we said earlier, the singular value decomposition should be
	thought of a generalization of orthogonal diagonalizations.
	For instance, the Spectral Theorem tells us that a symmetric
	matrix can be written as <m>QDQ^T</m>.  Many matrices,
	however, are not symmetric and so they are not orthogonally
	diagonalizable.  However, every matrix has a singular value
	decomposition <m>U\Sigma V^T</m>.  The price of this
	generalization is that we usually have two different
	orthogonal matrices <m>U</m> and <m>V</m> whereas we have only
	one orthogonal matrix <m>Q</m> in an orthogonal
	diagonalization. 
      </p>
	      
    </subsection>

    <subsection>
      <title> The structure of singular value decompositions </title>

      <p>
	Now that we have an understanding of what a singular value
	decomposition is and how to construct it, let's explore the
	ways in which a
	singular value decomposition reveals the underlying
	structure of the matrix.  As we'll see, the matrices <m>U</m>
	and <m>V</m> in a singular value decomposition provide
	convenient bases for some subspaces, such as the column and
	null spaces, of the matrix.  This observation will provide the
	key to some of our uses of these decompositions.
      </p>

      <activity>
	<statement>
	  <p>
	    Let's suppose that a matrix <m>A</m> has a singular value
	    decomposition <m>A=U\Sigma V^T</m> where
	    <me>
	      U=\begin{bmatrix}
	      \uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4
	      \end{bmatrix},\hspace{10pt}
	      \Sigma = \begin{bmatrix}
	      20 \amp 0 \amp 0 \\
	      0 \amp 5 \amp 0 \\
	      0 \amp 0 \amp 0 \\
	      0 \amp 0 \amp 0
	      \end{bmatrix},\hspace{10pt}
	      V=\begin{bmatrix}
	      \vvec_1 \amp \vvec_2 \amp \vvec_3
	      \end{bmatrix}.
	    </me>
	    <ol label="a.">
	      <li>
		<p>
		  What is the shape of <m>A</m>;  that is, how many
		  rows and columns does <m>A</m> have?
		</p>
	      </li>

	      <li>
		<p>
		  Suppose we write a three-dimensional vector
		  <m>\xvec</m> as a linear combination of right
		  singular vectors:
		  <me>
		    \xvec = c_1\vvec_1 + c_2\vvec_2 + c_3\vvec_3.
		  </me>
		  We would like to find an expression for
		  <m>A\xvec</m>.  To begin, find the vector
		  <m>V^T\xvec = \threevec{\vvec_1\cdot\xvec}
		  {\vvec_2\cdot\xvec}
		  {\vvec_3\cdot\xvec}
		  </m>.
		</p>
	      </li>

	      <li>
		<p>
		  Now multiply this vector by <m>\Sigma</m> to obtain
		  <m>\Sigma V^T\xvec</m>.
		</p>
	      </li>

	      <li>
		<p>
		  Finally, write <m>A\xvec = U(\Sigma V^T\xvec)</m> as
		  a linear combination of left singular vectors
		  <m>\uvec_j</m>.
		</p>
	      </li>

	      <li>
		<p>
		  We now have <m>A\xvec = 20c_1\uvec_1 +
		  5c_2\uvec_2</m>.  Find the vectors <m>\xvec</m> that
		  solve the equation <m>A\xvec = \uvec_1</m> by
		  finding the appropriate coefficients <m>c_1</m>,
		  <m>c_2</m>, <m>c_3</m>.  What does this tell us
		  about whether <m>\uvec_1</m> is in
		  <m>\col(A)</m>?
		</p>
	      </li>

	      <li>
		<p>
		  Find the vectors <m>\xvec</m> that solve the equation
		  <m>A\xvec=\uvec_2</m>.  What does this say about
		  whether <m>\uvec_2</m> is in <m>\col(A)</m>?
		</p>
	      </li>

	      <li>
		<p>
		  Find the vectors <m>\xvec</m> that solve the equation
		  <m>A\xvec=\uvec_3</m>.  What does this say about
		  whether <m>\uvec_3</m> is in <m>\col(A)</m>?
		</p>
	      </li>

	      <li>
		<p>
		  Find the vectors <m>\xvec</m> that solve the equation
		  <me>
		    A\xvec=3\uvec_1 - 7\uvec_2 + \uvec_3 - 4\uvec_4.
		  </me>
		</p>
	      </li>

	      <li>
		<p>
		  Find a basis for <m>\col(A)</m> consisting of left
		  singular vectors.  What is <m>\rank(A)</m>?
		</p>
	      </li>

	      <li>
		<p>
		  Remembering that <m>\col(A)^\perp = \nul(A^T)</m>,
		  find a basis for <m>\nul(A^T)</m> consisting of left
		  singular vectors.
		</p>
	      </li>

	      <li>
		<p>
		  For what vectors <m>\xvec</m> do we have <m>A\xvec =
		  \zerovec</m>?
		</p>
	      </li>

	      <li>
		<p>
		  Find a basis for <m>\nul(A)</m> consisting of right
		  singular vectors.
		</p>
	      </li>
	    </ol>
	  </p>
	</statement>
      </activity>

      <p>
	This activity shows how a singular value decomposition of a
	matrix encodes important information about its null and
	column spaces.  Using the example in the activity, we begin
	by remembering that <m>A</m> has the same shape as
	<m>\Sigma</m>, so it must have four rows and three columns.
      </p>

      <p>
	We now
	write a vector <m>\xvec</m> 
	in terms of left singular vectors,
	<m>\xvec=c_1\vvec_1+c_2\vvec_2 + c_3\vvec_3</m> so that
	<md>
	  <mrow>
	    A\xvec=\amp U\Sigma V^T\xvec = U\Sigma
	    \threevec{\vvec_1\cdot\xvec} {\vvec_2\cdot\xvec}
	    {\vvec_3\cdot\xvec}
	  </mrow>
	  <mrow>
	    =\amp U\begin{bmatrix} 20 \amp 0 \amp 0 \\
	    0 \amp 5 \amp 0 \\
	    0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0
	    \end{bmatrix}
	    \threevec{c_1}{c_2}{c_3}
	  </mrow>
	  <mrow>
	    =\amp
	    \begin{bmatrix}
	    \uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4
	    \end{bmatrix}
	    \fourvec{20c_1}{5c_2}00
	  </mrow>
	  <mrow>
	    =\amp 20c_1\uvec_1 + 5c_2\uvec_2.
	  </mrow>
	</md>
	We are left with
	<me>
	  A(c_1\vvec_1 + c_2\vvec_2 + c_3\vvec_3) = 20c_1\uvec_1 +
	  5c_2\uvec_2.
	</me>
      </p>

      <p>
	Suppose we write <m>\bvec</m>, a four-dimensional vector, as a
	linear combination
	of left singular vectors:
	<me>
	  \bvec = b_1\uvec_1 + b_2\uvec_2 + b_3\uvec_3 + b_4\uvec_4.
	</me>
	The equation <m>A\xvec = \bvec</m> then takes the form
	<me>
	  20c_1\uvec_1 + 5c_2\uvec_2 = b_1\uvec_1 + b_2\uvec_2 +
	  b_3\uvec_3 + b_4\uvec_4.
	</me>
	Because the left singular vectors form a basis of
	<m>\real^4</m>, we 
	find a solution by simply equating the coefficients on the two
	sides: 
	<md>
	  <mrow>
	    20c_1 \amp = b_1
	  </mrow>
	  <mrow>
	    5c_2 \amp = b_2
	  </mrow>
	  <mrow>
	    0 \amp = b_3
	  </mrow>
	  <mrow>
	    0 \amp = b_4.
	  </mrow>
	</md>
	In other words, this equation is only consistent if
	<m>b_3=b_4 = 0</m> in which case 
	<m>\bvec = b_1\uvec_1 +
	b_2\uvec_2</m>. Therefore, <m>A\xvec = \bvec</m> is consistent
	only when <m>\bvec</m> is a linear combination of
	<m>\uvec_1</m> and 
	<m>\uvec_2</m>, which implies that <m>\uvec_1</m> and
	<m>\uvec_2</m> form a basis for <m>\col(A)</m>.
      </p>

      <p>
	Because <m>\uvec_3</m> and <m>\uvec_4</m> are orthogonal to
	<m>\uvec_1</m> and <m>\uvec_2</m>, they must lie in
	<m>\col(A)^\perp = \nul(A^T)</m>.  We therefore see that
	<m>\uvec_3</m> and <m>\uvec_4</m> form a basis for
	<m>\nul(A^T)</m>.
      </p>

      <p>
	Finally, remember that <m>\nul(A)</m> is the solution set of
	the equation
	<me>
	  A\xvec = \zerovec = 
	  20c_1\uvec_1 + 5c_2\uvec_2.
	</me>
	If <m>\xvec</m> is a
	solution, we must have <m>c_1=c_2=0</m> so <m>\xvec</m> has
	the form <m>\xvec=c_3\vvec_3</m>.  This shows that
	<m>\vvec_3</m> is a basis for <m>\nul(A)</m>.
      </p>

      <p>
	In this way, we see that the left and right singular vectors
	form bases for <m>\col(A)</m>, <m>\nul(A^T)</m>, and
	<m>\nul(A)</m>.  Notice that there are two vectors in a basis
	for <m>\col(A)</m>, which tells us that <m>\rank(A) =
	\dim\col(A) = 2</m>.  In fact, we have one basis vector for
	every nonzero singular value meaning that, more generally,
	<m>\rank(A)</m> is the number of nonzero singular values.
      </p>

      <p>
	Generally speaking, if the rank of a matrix <m>A</m> is
	<m>r</m>, then <m>\Sigma</m> has the form
	<me>
	  \begin{bmatrix}
	  \sigma_1 \amp \ldots \amp 0 \amp \ldots \amp 0 \\
	  0 \amp \ldots \amp 0 \amp \ldots \amp 0 \\
	  0 \amp \ldots \amp \sigma_r \amp \ldots \amp 0 \\
	  0 \amp \ldots \amp 0 \amp \ldots \amp 0 \\
	  0 \amp \vdots \amp 0 \amp \vdots \amp 0 \\
	  0 \amp \ldots \amp 0 \amp \ldots \amp 0 \\
	  \end{bmatrix},
	</me>
	The columns of
	<m>U</m> contain bases for <m>\col(A)</m> and
	<m>\nul(A^T)</m>,
	<me>
	  U = \left[
	  \underbrace{\uvec_1 ~ \ldots ~ \uvec_r}_{\col(A)} \hspace{3pt}
	  \underbrace{\uvec_{r+1} ~ \ldots ~ \uvec_m}_{\nul(A^T)}
	  \right]
	</me>
	and the columns of <m>V</m> contain a basis for <m>\nul(A)</m>:
	<me>
	  V = \left[
	  \vvec_1 ~ \ldots ~ \vvec_r\hspace{3pt}
	  \underbrace{\vvec_{r+1} ~ \ldots ~ \vvec_m}_{\nul(A)}
	  \right]
	</me>
      </p>

      <p>
	If you're bothered by the fact that the first <m>r</m> columns
	of <m>V</m> don't seem to describe a natural subspace, don't
	despair.  Remember that writing <m>A=U\Sigma V^T</m> means
	that <m>A^T=V\Sigma^T U^T</m> so that the right singular
	vectors of <m>A</m> become left singular vectors of
	<m>A^T</m>.  Since <m>\rank(A^T) = \rank(A) = r</m>, it
	follows that <m>\vvec_1, \ldots, \vvec_r</m> form a basis for
	<m>\col(A^T)</m> so we have
	<me>
	  V = \left[
	  \underbrace{\vvec_1 ~ \ldots ~ \vvec_r}_{\col(A^T)} \hspace{3pt}
	  \underbrace{\vvec_{r+1} ~ \ldots ~ \vvec_m}_{\nul(A)}
	  \right].
	</me>
      </p>

      <p>
	Because the columns of <m>A^T</m> are the rows of <m>A</m>, this
	subspace is sometimes called the <em> row space </em> of
	<m>A</m> and denoted <m>\row(A)</m>.  While we haven't had an
	occasion to use <m>\row(A)</m> so far, there are times when
	it's important to have an orthonormal basis for it.
	Fortunately, a singular value decomposition provides just that.
      </p>

      <p>
	Considered altogether, the subspaces <m>\col(A)</m>,
	<m>\nul(A)</m>, <m>\col(A^T)</m>, and <m>\nul(A^T)</m> are
	called the <em>four fundamental subspaces </em> associated to
	<m>A</m>.  In addition to telling us the rank of a matrix, a
	singular value decomposition gives us orthonormal bases for
	all four fundamental subspaces.
      </p>

      <theorem xml:id="thm-four-subspaces">
	<statement>
	  <p>
	    Suppose <m>A</m> is an <m>m\times n</m> matrix having a
	    singular value decomposition <m>A=U\Sigma V^T</m>.  Then
	    <ul>
	      <li>
		<p>
		  <m>r=\rank(A)</m> is the number of nonzero singular
		  values.
		</p>
	      </li>
	      <li>
		<p>
		  The columns <m>\uvec_1,\uvec_2,\ldots,\uvec_r</m>
		  form an orthonormal basis for <m>\col(A)</m>.
		</p>
	      </li>
	      <li>
		<p>
		  The columns <m>\uvec_{r+1},\ldots,\uvec_m</m> form
		  an orthonormal basis for <m>\nul(A^T)</m>.
		</p>
	      </li>
	      <li>
		<p>
		  The columns <m>\vvec_1,\vvec_2,\ldots,\vvec_r</m>
		  form an orthonormal basis for <m>\col(A^T)</m>.
		</p>
	      </li>
	      <li>
		<p>
		  The columns <m>\vvec_{r+1},\ldots,\vvec_n</m> form
		  an orthonormal basis for <m>\nul(A)</m>.
		</p>
	      </li>
	    </ul>
	  </p>
	</statement>
      </theorem>

      <p>
	When we previously outlined a procedure for finding a singular
	decomposition of an <m>m\times n</m> matrix <m>A</m>, we found
	the left singular vectors <m>\uvec_j</m> using the expression
	<m>A\vvec_j = \sigma_j\uvec_j</m>.  This produces left
	singular vectors <m>\uvec_1, \uvec_2,\ldots,\uvec_r</m>, where
	<m>r=\rank(A)</m>.  If <m>r\lt m</m>, however, we still need
	to find the left singular vectors
	<m>\uvec_{r+1},\ldots,\uvec_m</m>.  <xref
	ref="thm-four-subspaces" /> tells us how to do that: because
	those vectors form an orthonormal basis for <m>\nul(A^T)</m>,
	we can find them by solving <m>A^T\xvec = \zerovec</m> to find
	a basis for <m>\nul(A^T)</m> and
	applying the Gram-Schmidt algorithm.
      </p>

      <p>
	We won't worry about this issue too much, however, as we will
	frequently use software to find singular value decompositions
	for us.
      </p>
		
    </subsection>

    <subsection>
      <title> Summary </title>

      <p>
	This section has explored singular value decompositions, how
	to find them, and how they organize important information
	about a matrix.
	<ul>
	  <li>
	    <p>
	      A singular value decomposition of a matrix <m>A</m> is
	      a factorization where <m>A=U\Sigma V^T</m>.  The matrix
	      <m>\Sigma</m> has the same shape as <m>A</m>, and its
	      only nonzero entries are the singular values of
	      <m>A</m>, which appear in decreasing order on the
	      diagonal.  The matrices <m>U</m> and <m>V</m> are
	      orthogonal and contain the left and right singular
	      vectors.
	    </p>
	  </li>

	  <li>
	    <p>
	      To find a singular value decomposition of a matrix, we
	      construct the Gram matrix <m>G=A^TA</m>, which is
	      symmetric.  The singular 
	      values of <m>A</m> are the square roots of the
	      eigenvalues of <m>G</m>, and the right singular vectors
	      <m>\vvec_j</m> are the associated eigenvectors of
	      <m>G</m>.  The left singular vectors <m>\uvec_j</m> are
	      determined from the relationship
	      <m>A\vvec_j=\sigma_j\uvec_j</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      A singular value decomposition organizes fundamental
	      information about a matrix.  For instance, the number of
	      nonzero singular values is the rank <m>r</m> of the
	      matrix.  The first <m>r</m> left singular vectors form
	      an orthonormal basis for <m>\col(A)</m> with the
	      remaining left singular vectors forming an orthonormal
	      basis of <m>\nul(A^T)</m>.  The first <m>r</m> right
	      singular vectors form an orthonormal basis for
	      <m>\col(A^T)</m> while the remaining right singular
	      vectors form an orthonormal basis of <m>\nul(A)</m>.
	    </p>
	  </li>
	</ul>
      </p>
    </subsection>

   <xi:include href="exercises/exercises7-4.xml" />
   
</section>

