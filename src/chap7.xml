<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="chap7" xmlns:xi="http://www.w3.org/2001/XInclude">

  <title> Singular Value Decompositions </title>

  <introduction>
    <p>
      <xref ref="chap4" /> demonstrated several uses of eigenvectors
      and eigenvalues.  For example, knowing the eigenvalues and
      eigenvectors of a matrix <m>A</m> helped us make predictions
      about the long-term behavior of dynamical systems in which some
      initial state <m>\xvec_0</m> evolved according to the rule
      <m>\xvec_{k+1} = A\xvec_k</m>.
    </p>

    <p>
      The range of problems we can study using eigenvectors is
      somewhat limited.  First, eigenvectors only exist when the
      matrix <m>A</m> is square, and we have seen situations, such as
      the least squares problems in <xref ref="sec-least-squares" />,
      where the matrices we're interested in are not square.  Second,
      even when <m>A</m> is square, there may not be a basis for
      <m>\real^n</m> consisting of eigenvectors of <m>A</m>, and this
      was an important condition we required for some of our work.
    </p>

    <p>
      This chapter introduces singular value decompositions, which
      provide a kind of generalization of eigenvectors and
      eigenvalues.  In fact, we will see that every matrix, whether
      square or not, has a
      singular value decomposition and knowing it gives us a great
      deal of insight into the matrix.  It's been said that having a
      singular value decomposition is like looking at a matrix with
      X-ray vision, as the decomposition reveals all the essential
      features of the matrix.
    </p>

  </introduction>
  
  <xi:include href="./section7-1.xml" />
  <xi:include href="./section7-2.xml" />
  <xi:include href="./section7-3.xml" />
  <xi:include href="./section7-4.xml" />
  
</chapter>
