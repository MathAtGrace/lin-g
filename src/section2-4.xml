<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-linear-dep"
	 xmlns:xi="http://www.w3.org/2001/XInclude">

  <title>
    Linear independence
  </title>

  <introduction>
    <p>
      In the previous section, we studied our question concerning the
      existence of solutions to a linear system by inquiring about the
      span of a set of vectors.  In particular, the span of a set of
      vectors <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m> is the set of
      vectors <m>\bvec</m> for which a solution to the linear system
      <m> \left[\begin{array}{rrrr}
      \vvec_1\amp\vvec_2\amp\ldots\amp\vvec_n
      \end{array}\right]
      \xvec
      =
      \bvec
      </m> exists.  
    </p>

    <p>
      In this section, our focus turns to the uniqueness of
      solutions of a linear system, the second of our two fundamental
      questions asked in <xref ref="fundamental-questions" />.  This
      will lead us to the concept of linear independence.
    </p>

  </introduction>

  <subsection>
    <title> Linear dependence </title>
    <p>
      In the previous section, we looked at some examples of the span
      of sets of vectors in <m>\real^3</m>.  We saw one example in
      which the span of three vectors formed a plane.  In another, the
      span of three vectors formed <m>\real^3</m>.  We would like to
      understand the difference in these two examples.
    </p>
    
    <exploration>
      <statement>
      <p>
	Let's start this activity by studying
	the span of two different sets of vectors.
      <ol label="a.">
	<li> <p>
	  Consider the following vectors in <m>\real^3</m>:
	  <me>
	    \vvec_1=\threevec{0}{-1}{2},
	    \vvec_2=\threevec{3}{1}{-1},
	    \vvec_3=\threevec{2}{0}{1}
	  </me>.
	  Describe the span of these vectors,
	  <m>\laspan{\vvec_1,\vvec_2,\vvec_3}</m>. 
	  <sage>
	    <input>
	    </input>
	  </sage>
	</p></li>

	<li><p>
	  We will now consider a set of vectors that looks very much
	  like the first set:
	  <me>
	    \wvec_1=\threevec{0}{-1}{2},
	    \wvec_2=\threevec{3}{1}{-1},
	    \wvec_3=\threevec{3}{0}{1}
	  </me>.
	  Describe the span of these vectors,
	  <m>\laspan{\wvec_1,\wvec_2,\wvec_3}</m>. 
	  <sage>
	    <input>
	    </input>
	  </sage>
	</p></li>

	<li><p> 
	  Show that the vector <m>\wvec_3</m> is a linear combination
	  of <m>\wvec_1</m> and <m>\wvec_2</m> by finding weights such
	  that
	  <me>
	    \wvec_3 = a\wvec_1 + b\wvec_2
	  </me>.
	</p></li>

	<li><p>
	  Explain why any linear combination of <m>\wvec_1</m>,
	  <m>\wvec_2</m>, and <m>\wvec_3</m>,
	  <me>
	    c_1\wvec_1 + c_2\wvec_2 + c_3\wvec_3
	  </me>
	  can be written as a linear combination of <m>\wvec_1</m> and
	  <m>\wvec_2</m>. 
	</p></li>

	<li><p>
	  Explain why
	  <me>
	    \laspan{\wvec_1,\wvec_2,\wvec_3} = \laspan{\wvec_1,\wvec_2}
	  </me>.
	</p></li>

      </ol>
      </p>
      </statement>

      <solution>
	<p> The solutions to this preview activity are given in the
	text below.
	</p>
      </solution>
    </exploration>

    <p>
      The preview activity presents us with two similar examples
      that demonstrate quite different behaviors.  In the first
      example, the vectors <m>\vvec_1</m>, <m>\vvec_2</m>, and
      <m>\vvec_3</m> span <m>\real^3</m>, which we recognize because
      the matrix
      <m>\left[\begin{array}{rrr}\vvec_1\amp\vvec_2\amp\vvec_3
      \end{array}\right]
      </m> has a pivot position in every row so that  <xref
      ref="prop-pivot-row" /> applies.
    </p>

    <p>
      However, the second example is very different.  In this case,
      the matrix <m>\left[\begin{array}{rrr}
      \wvec_1\amp\wvec_2\amp\wvec_3 \end{array}\right]
      </m> has only two pivot positions:
      <me>
	\left[\begin{array}{rrr}
	\wvec_1 \amp \wvec_2 \amp \wvec_3
	\end{array}\right]
	=
	\left[\begin{array}{rrr}
	0 \amp 3 \amp 3 \\
	-1 \amp 1 \amp 0 \\
	2 \amp -1 \amp 1
	\end{array}\right]
	\sim
	\left[\begin{array}{rrr}
	1 \amp 0 \amp 1 \\
	0 \amp 1 \amp 1 \\
	0 \amp 0 \amp 0
	\end{array}\right]
      </me>.
    </p>

    <p> Let's look 
      at this matrix and change our perspective slightly by
      considering it to be an augmented matrix.
      <me>
	\left[\begin{array}{rr|r}
	\wvec_1 \amp \wvec_2 \amp \wvec_3
	\end{array}\right]
	=
	\left[\begin{array}{rr|r}
	0 \amp 3 \amp 3 \\
	-1 \amp 1 \amp 0 \\
	2 \amp -1 \amp 1
	\end{array}\right]
	\sim
	\left[\begin{array}{rr|r}
	1 \amp 0 \amp 1 \\
	0 \amp 1 \amp 1 \\
	0 \amp 0 \amp 0
	\end{array}\right]
      </me>
      By doing so, we seek to express <m>\wvec_3</m> as a
      linear combination of <m>\wvec_1</m> and <m>\wvec_2</m>.  In
      fact, the reduced row echelon form shows us that
      <me>
	\wvec_3 = \wvec_1 + \wvec_2
      </me>.
    </p>

    <p>
      Consequently, we can rewrite any linear cominbation of
      <m>\wvec_1</m>, <m>\wvec_2</m>, and <m>\wvec_3</m> so that
      <me>
	\begin{aligned}
	c_1\wvec_1 + c_2\wvec_2 + c_3\wvec_3 \amp
	{}={}
	c_1\wvec_1 + c_2\wvec_2 + c_3(\wvec_1+\wvec_2) \\
	\amp {}={}
	(c_1+c_3)\wvec_1 + (c_2+c_3)\wvec_2 \\
	\end{aligned}
      </me>.
      In other words, any linear combination of <m>\wvec_1</m>,
      <m>\wvec_2</m>, and <m>\wvec_3</m> may be written as a linear
      combination using only the vectors <m>\wvec_1</m> and
      <m>\wvec_2</m>.  Since the span of a set of vectors is simply
      the set of their linear combinations, this shows that
      <me>
	\laspan{\wvec_1,\wvec_2,\wvec_3} = \laspan{\wvec_1,\wvec_2}
      </me>.
      In other words, adding the vector <m>\wvec_3</m> to the set of
      vectors <m>\wvec_1</m> and <m>\wvec_2</m> does not change the
      span.
    </p>

    <p> Before exploring this type of behavior more generally,
    let's think about this from a geometric point of view.  In the
    first example, we begin with two vectors <m>\vvec_1</m> and
    <m>\vvec_2</m> and add a third vector <m>\vvec_3</m>.
    <sidebyside widths="45% 45%">
      <image source="images/3d-plane-nonstd" />
      <image source="images/3d-plane-span-no-w" />
    </sidebyside>
    Because the vector <m>\vvec_3</m> is not a linear combination of
    <m>\vvec_1</m> and <m>\vvec_2</m>, it provides a direction to move
    that, 
    when creating linear combinations, is independent of
    <m>\vvec_1</m> and <m>\vvec_2</m>.  The three vectors therefore
    span <m>\real^3</m>.
    </p>

    <p>
      In the second example, however, the third vector <m>\wvec_3</m>
      is a linear combination of <m>\wvec_1</m> and <m>\wvec_2</m> so
      it already lies in the plane formed by these two vectors.
      <sidebyside widths="45% 45%">
	<image source="images/3d-plane-nonstd-w" />
	<image source="images/3d-plane-span-yes-w" />
      </sidebyside>
      Since we can already move in this direction with just the first
      two vectors <m>\wvec_1</m> and <m>\wvec_2</m>, adding
      <m>\wvec_3</m> to the set does not enlarge the span.  It remains
      a plane.
    </p>

    <p> With these examples in mind, we will make the following
    definition.
    </p>

    <definition>
      <idx> linearly independent </idx>
      <idx> linearly dependent </idx>
      <statement>
	<p> A set of vectors <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m> is
	called <em> linearly dependent </em> if one of the vectors is
	a linear combination of the others.  Otherwise, the set of
	vectors is called <em> linearly independent</em>.
	</p>
      </statement>
    </definition>

    <p> For the sake of completeness, we say that a set of vectors
    containing only one vector is linearly independent if that vector
    is not the zero vector, <m>\zerovec</m>.
    </p>

  </subsection>

  <subsection>
    <title> How to recognize linear dependence </title>

    <activity>
      <statement>
      <p> We would like to develop a means of detecting when a set of
      vectors is linearly dependent.  These questions will point the
      way. 
      <ol label="a.">
	<li ><p> Suppose we have five vectors in <m>\real^4</m> that
	form the columns of a matrix having reduced row echelon form
	<me>
	  \left[\begin{array}{rrrrr}
	  \vvec_1 \amp 	  \vvec_2 \amp 	  \vvec_3 \amp
	  \vvec_4 \amp	  \vvec_5 
	  \end{array}\right]
	  \sim
	  \left[\begin{array}{rrrrr}
	  1 \amp 0 \amp -1 \amp 0 \amp 2 \\
	  0 \amp 1 \amp 2 \amp 0 \amp 3 \\
	  0 \amp 0 \amp 0 \amp 1 \amp -1 \\
	  0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	  \end{array}\right]
	</me>.
	Is it possible to write one of the vectors
	<m>\vvec_1,\vvec_2,\ldots,\vvec_5</m> as a linear
	combination of the others?  If so, show explicitly how one
	vector appears as a linear combination of some of the other
	vectors.  Is this set of vectors linearly dependent or
	independent? 
	</p></li>

	<li><p> Suppose we have another set of three vectors in
	<m>\real^4</m> that form the columns of a matrix having 
	reduced row echelon form
	<me>
	  \left[\begin{array}{rrr}
	  \wvec_1 \amp 	  \wvec_2 \amp 	  \wvec_3 \\
	  \end{array}\right]
	  \sim
	  \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  0 \amp 0 \amp 0 \\
	  \end{array}\right]
	</me>.
	Is it possible to write one of these vectors <m>\wvec_1</m>,
	<m>\wvec_2</m>, <m>\wvec_3</m> as a linear
	combination of the others?  If so, show explicitly how one
	vector appears as a linear combination of some of the other
	vectors.  Is this set of vectors linearly dependent or
	independent? 
	</p></li>

	<li><p>
	  By looking at the pivot positions, how can you determine
	  whether the columns of a matrix are linearly dependent or
	  independent?
	</p></li>

	<li><p>
	  If one vector in a set is the zero vector <m>\zerovec</m>,
	  can the set of vectors be linearly independent?
	</p></li>

	<li><p>
	  Suppose a set of vectors in <m>\real^{10}</m> has twelve
	  vectors.  Is it possible for this set to be linearly
	  independent?
	</p></li>
      </ol></p>
      </statement>

      <solution>
	<p><ol label="a.">
	  <li><p>
	    Let's focus on the first three vectors and view the matrix
	    as an augmented one:
	    <me>
	      \left[\begin{array}{rr|r}
	      \vvec_1 \amp \vvec_2 \amp \vvec_3
	      \end{array}\right]
	      \sim
	      \left[\begin{array}{rr|r}
	      1 \amp 0 \amp -1 \\
	      0 \amp 1 \amp 2 \\
	      0 \amp 0 \amp 0 \\
	      0 \amp 0 \amp 0 \\
	      \end{array}\right]\text{.}
	    </me>
	    This shows that <m>\vvec_3=-\vvec_1+2\vvec_2</m> so it is
	    possible to write one of the vectors as a linear
	    combination of the others.  We can also see that
	    <m>\vvec_5 = 2\vvec_1+3\vvec_2-\vvec_4</m> in the same
	    way.
	  </p></li>

	  <li><p>
	    Applying the same reasoning as in the previous part, we
	    see that we cannot write any of the vectors as a linear
	    combination of the others.
	  </p></li>

	  <li><p>
	    The columns of a matrix are linearly independent exactly
	    when there is a pivot position in every column of the
	    matrix.
	  </p></li>

	  <li><p>
	    Yes, because we can write <m>\zerovec = 0\vvec_1 + \ldots
	    + 0\vvec_n</m>.
	  </p></li>

	  <li><p>
	    No, because the matrix formed by the vectors would have 12
	    columns and only 10 rows.  There can at most be 10 pivot
	    positions so there are at least two columns without pivot
	    positions.
	  </p></li>
	</ol></p>
      </solution>
    </activity>

    <p>
      By now, it shouldn't be too surprising that the pivot positions
      play an important role in determining whether the columns of a
      matrix are linearly dependent.  Let's discuss the previous
      activity to make this clear.
      
    <ul>
      <li> <p>
	Let's consider the first example
	from the activity in which we have vectors in <m>\real^4</m>
	such that
	<me>
	  \left[\begin{array}{rrrrr}
	  \vvec_1 \amp 	  \vvec_2 \amp 	  \vvec_3 \amp
	  \vvec_4 \amp	  \vvec_5 
	  \end{array}\right]
	  \sim
	  \left[\begin{array}{rrrrr}
	  1 \amp 0 \amp -1 \amp 0 \amp 2 \\
	  0 \amp 1 \amp 2 \amp 0 \amp 3 \\
	  0 \amp 0 \amp 0 \amp 1 \amp -1 \\
	  0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	  \end{array}\right]
	</me>.
	Notice that the third column does not contain a pivot position.
	Let's focus on the first three columns and consider them as an
	augmented matrix:
	<me>
	  \left[\begin{array}{rr|r}
	  \vvec_1 \amp 	  \vvec_2 \amp 	  \vvec_3 
	  \end{array}\right]
	  \sim
	  \left[\begin{array}{rr|r}
	  1 \amp 0 \amp -1 \\
	  0 \amp 1 \amp 2 \\
	  0 \amp 0 \amp 0 \\
	  0 \amp 0 \amp 0 \\
	  \end{array}\right]
	</me>.
	There is not a pivot in the rightmost column so we know that 
	<m>\vvec_3</m> can be written as a linear combination of
	<m>\vvec_1</m> and <m>\vvec_2</m>.  In fact, we can read the
	weights from the augmented matrix:
	<me>
	\vvec_3 = -\vvec_1 + 2\vvec_2
	</me>.
	This says that the set of vectors
	<m>\vvec_1,\vvec_2,\ldots,\vvec_5</m> is linearly dependent.
      </p>

      <p>
	This points to the general observation that a set of vectors is
	linearly dependent if the matrix they form has a column without a
	pivot.
      </p>

      <p>
	In addition, the fifth column of this matrix does not contain a
	pivot meaning that <m>\vvec_5</m> can be written as a linear
	combination:
	<me>
	  \vvec_5 = 2\vvec_1 + 3\vvec_2 -\vvec_4
	  </me>.
      </p>

      <p> This example shows that vectors in columns that do not contain
      a pivot may be expressed as a linear combination of the vectors in
      columns that do contain pivots.  In this case, we have
      <me>
	\laspan{\vvec_1,\vvec_2,\vvec_3,\vvec_4,\vvec_5}
	=\laspan{\vvec_1,\vvec_2,\vvec_4}
	</me>.
      </p> </li>

      <li><p>
	Conversely, the second set of vectors we studied produces a
	matrix with a pivot in every column.
	<me>
	  \left[\begin{array}{rrr}
	  \wvec_1 \amp 	  \wvec_2 \amp 	  \wvec_3 \\
	  \end{array}\right]
	  \sim
	  \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  0 \amp 0 \amp 0 \\
	  \end{array}\right]
	  </me>.
	  If we interpret this as an augmented matrix again, we see
	  that the linear system is inconsistent since there is a pivot in
	  the rightmost column.  This means that <m>\wvec_3</m> cannot be
	  expressed as a linear combination of <m>\wvec_1</m> and
	  <m>\wvec_2</m>.
	</p>

	<p>
	  Similarly, <m>\wvec_2</m> cannot be expressed as a linear
	  combination of <m>\wvec_1</m>. In addition, if <m>\wvec_2</m>
	  could be expressed as a linear combination of <m>\wvec_1</m> and
	  <m>\wvec_3</m>, we could rearrange that expression to write
	  <m>\wvec_3</m> as a linear combination of <m>\wvec_1</m> and
	  <m>\wvec_2</m>, which we know is impossible.
	</p>
	
	<p>
	  We can therefore conclude that <m>\wvec_1</m>, <m>\wvec_2</m>,
	  and <m>\wvec_3</m> form a linearly indpendent set of vectors.
      </p></li>
    </ul>
    </p>
    
    <p> This leads to the following proposition. </p>

    <proposition>
      <statement>
	<p> The columns of a matrix are linearly independent if and
	only if every column contains a pivot position.
	</p>
      </statement>
    </proposition>

    <p>
      This condition imposes a constraint on how many vectors we can
      have in a linearly independent set.  Here is an example of the
      reduced row echelon form of a matrix having linearly independent
      columns.  Notice that there are three vectors in <m>\real^5</m>
      so there are at least as many rows as columns.

      <me>
	\left[\begin{array}{rrr}
	1 \amp 0 \amp 0 \\
	0 \amp 1 \amp 0 \\
	0 \amp 0 \amp 1 \\
	0 \amp 0 \amp 0 \\
	0 \amp 0 \amp 0 \\
	\end{array}\right]
      </me>.
    </p>

    <p>
      More generally, suppose that
      <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m> is a 
      linearly independent set of vectors in <m>\real^m</m>.  When
      these vectors form the columns of a matrix, there must be a
      pivot position in every column of the matrix.  Since every row
      contains at most one pivot position, the number of columns can
      be no greater than the number of rows.
      This means that the number of vectors in a linearly independent
      set can be no greater than the number of dimensions.
    </p>

    <proposition>
      <statement>
	<p> A linearly independent set of vectors in <m>\real^m</m>
	can contain no more than <m>m</m> vectors.
	</p>
      </statement>
    </proposition>
	
    <p>
      This says, for instance, that any linearly independent
      set of vectors in <m>\real^3</m> can contain no more three
      vectors.  Once again, this makes intuitive sense.  We usually
      imagine three independent directions, such as up/down,
      front/back, left/right, in our three-dimensional world.  This
      proposition tells us that there can be no more independent
      directions.
    </p>

  </subsection>

  <subsection>
    <title> The homogeneous equation </title>

    <p>
      Given an <m>m\times n</m> matrix <m>A</m>, we call the equation
      <m>A\xvec = \zerovec</m> a <em>homogenous</em> equation.  The
      solutions to this equation reflect whether the columns of
      <m>A</m> are linearly independent or not.
    </p>

    <activity>
      <title> Linear independence and homogeneous equations </title>

      <statement>
      <p><ol label="a.">
	<li><p> Explain why the homogenous equation <m>A\xvec =
	\zerovec</m> is consistent no matter the matrix <m>A</m>.
	</p></li>

	<li><p> Consider the matrix
	<me>
	A = \left[\begin{array}{rrr}
	3 \amp 2 \amp 0 \\
	-1 \amp 0 \amp -2 \\
	2 \amp 1 \amp 1
	\end{array}\right]
	</me>
	whose columns we denote by <m>\vvec_1</m>, <m>\vvec_2</m>, and
	<m>\vvec_3</m>.
	Are the vectors <m>\vvec_1</m>, <m>\vvec_2</m>, and
	<m>\vvec_3</m> linearly dependent or independent?
	<sage>
	  <input>
	  </input>
	</sage>
	      
	</p></li>

	<li><p>
	  Give a description of the solution space of
	  the homogeneous equation <m>A\xvec = \zerovec</m>.
	</p></li>

	<li><p>
	  We know that <m>\zerovec</m> is a solution to the
	  homogeneous equation.  Find another solution that is
	  different from <m>\zerovec</m>.  Use your solution to find
	  weights 
	  <m>c_1</m>, <m>c_2</m>, and <m>c_3</m> such that
	  <me>
	    c_1\vvec_1 + c_2\vvec_2 + c_3\vvec_3 = \zerovec
	  </me>.
	</p></li>

	<li><p>
	  Use the expression you found in the previous part to write
	  one of the vectors as a linear combination of the others.
	</p></li>
      </ol></p>
      </statement>

      <solution>
	<p><ol label="a.">
	  <li><p>
	    The vector <m>\zerovec</m> is always a solution.
	  </p></li>

	  <li><p>
	    We have
	    <me>
	      \left[\begin{array}{rrr}
	      3 \amp 2 \amp 0 \\
	      -1 \amp 0 \amp -2 \\
	      2 \amp 1 \amp 1
	      \end{array}\right]
	      \sim
	      \left[\begin{array}{rrr}
	      1 \amp 0 \amp 2 \\
	      0 \amp 1 \amp -3 \\
	      0 \amp 0 \amp 0
	      \end{array}\right]\text{,}
	    </me>
	    which shows that the vectors are linearly dependent.  
	  </p></li>

	  <li><p>
	    From the reduced row echelon form, we see that <m>x_3</m>
	    is a free variable and that we have
	    <me>
	      \begin{alignedat}{2}
	      x_1 \amp {}={} -2x_3 \\
	      x_2 \amp {}={} 3x_3\text{.} \\
	      \end{alignedat}
	    </me>
	    The solution space is then written parametrically as
	    <me>
	      \xvec=\threevec{x_1}{x_2}{x_3} = x_3\threevec{-2}{3}{1}\text{.}
	    </me>
	  </p></li>

	  <li><p>
	    If we set <m>x_3=1</m>, then we have the solution
	    <m>\xvec=\threevec{-2}{3}{1}</m>, which says that
	    <me>
	      -2\vvec_1+3\vvec_2+\vvec_3 = \zerovec\text{.}
	    </me>
	  </p></li>

	  <li><p>
	    We may rewrite this expression as
	    <m>\vvec_3=2\vvec_1-3\vvec_2</m>, showing that
	    <m>\vvec_3</m> is a linear combination of <m>\vvec_1</m>
	    and <m>\vvec_2</m>.
	  </p></li>
	  </ol></p>
      </solution>
	      
    </activity>

    <p>
      <idx> trivial solution </idx>
      <idx> nontrivial solution </idx>
      For any matrix <m>A</m>, we know that the equation <m>A\xvec =
      \zerovec</m> has at least one solution, namely, the vector
      <m>\xvec = \zerovec</m>.  We call this the trivial solution to
      the homogeneous equation so that any other solution that exists
      is a <m>nontrivial</m> solution.
    </p>

    <p>
      If there is no nontrivial solution, then <m>A\xvec =
      \zerovec</m> has exactly one solution.  There can be no free
      variables in a description of the solution space so <m>A</m>
      must have a pivot position in every column.  In this case, the
      columns of <m>A</m> must be linearly independent.
    </p>

    <p>
      If, however, there is a nontrivial solution, then there are
      infinitely many solutions so <m>A</m> must
      have a column without a pivot position.  Hence, the
      columns of <m>A</m> are linearly dependent.
    </p>

    <example>
      <statement>
	<p>
      We will make the connection between solutions to the homogeneous
      equation and the linear independence of the columns more explict
      by looking at an example.  In particular, we will demonstrate
      how a nontrivial solution to the homogeneous equation shows that
      one column of <m>A</m> is a linear combination of the others.
      With the matrix <m>A</m> in the previous activity, the
      homogeneous equation has the reduced row echelon form
      <me>
	\left[\begin{array}{rrr|r}
	3 \amp 2 \amp 0 \amp 0 \\
	-1 \amp 0 \amp -2 \amp 0 \\
	2 \amp 1 \amp 1 \amp 0 \\
	\end{array}\right]
	\sim
	\left[\begin{array}{rrr|r}
	1 \amp 0 \amp 2 \amp 0 \\
	0 \amp 1 \amp -3 \amp 0 \\
	0 \amp 0 \amp 0 \amp 0 \\
	\end{array}\right]
      </me>,
      which implies that
      <me>
	\begin{alignedat}{4}
	x_1 \amp \amp \amp {}+{} \amp 2x_3 \amp {}={} \amp 0 \\
	\amp \amp x_2 \amp {}-{} \amp 3x_3 \amp {}={} \amp 0 \\
	\end{alignedat}
      </me>.
      In terms of the free variable <m>x_3</m>, we have
      <me>
	\begin{aligned}
	x_1 \amp {}={} -2x_3 \\
	x_2 \amp {}={} 3x_3 \\
	\end{aligned}
      </me>.
      Any choice for a value of the free variable <m>x_3</m> produces
      a solution so let's choose, for convenience, <m>x_3=1</m>.
      We then have <m>(x_1,x_2,x_3) = (-2,3,1)</m>.
    </p>

    <p>
      Since <m>(-2,3,1)</m> is a solution to the homogeneous equation
      <m>A\xvec=\zerovec</m>, this solution gives weights for a
      linear combination of the columns of <m>A</m> that create
      <m>\zerovec</m>.  That is,
      <me>
	-2\vvec_1 + 3\vvec_2 + \vvec_3 = \zerovec
      </me>,
      which we rewrite as
      <me>
	\vvec_3 = 2\vvec_1 - 3\vvec_2
      </me>.
    </p>
      </statement>
    </example>

    <p>
      As this example demonstrates, there are many ways we can view
      the question of linear independence.  We will record some of
      these ways in the following proposition.
    </p>

    <proposition>
      <statement>
	<p> For a matrix <m>A = \left[\begin{array}{rrrr}
	\vvec_1\amp\vvec_2\amp\ldots\amp\vvec_n
	\end{array}\right]
	</m>,
	the following statements are equivalent:
	<ul>
	  <li> <p> The columns of <m>A</m> are linearly
	  dependent. </p></li>

	  <li> <p> One of the vectors in the set
	  <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m> is a linear
	  combination of the others. </p></li>

	  <li> <p> The matrix <m>A</m> has a column without a pivot
	  position. </p></li>

	  <li> <p> The homogeneous equation <m>A\xvec = \zerovec</m>
	  has a nontrivial solution. </p></li>

	  <li> <p> There are weights <m>c_1,c_2,\ldots,c_n</m>, not
	  all of which are zero, such that
	  <me>
	    c_1\vvec_1 + c_2\vvec_2 + \ldots + c_n\vvec_n = \zerovec
	  </me>. </p></li>
	</ul>
	</p>
      </statement>
    </proposition>
	  
  </subsection>

  <subsection>
    <title>
      Summary
    </title>

    <p> In this section, we developed the concept of linear dependence
    of a set of vectors.  At the beginning of the section, we said
    that this concept addressed the second of our fundamental
    questions, expressed in <xref ref="fundamental-questions" />,
    concerning the uniqueness of solutions to a linear system.  It is
    worth comparing the results of this section with those of the
    previous one so that the parallels between them become clear.
    </p>

    <p>
    As is usual, we will write a matrix as a collection
    of vectors,
    <me>A=\left[\begin{array}{rrrr} \vvec_1\amp\vvec_2 \amp
    \ldots \amp \vvec_n \end{array}\right].
    </me>
    </p>

    <p>
      <dl>
	<li><title> Existence </title>
	<p>
	  In the previous section, we asked if we could write a vector
	  <m>\bvec</m> as a linear combination of the columns of
	  <m>A</m>, which happens precisely when a solution to the
	  equation <m>A\xvec = \bvec</m> exists.  We saw that every vector
	  <m> \bvec</m> could be expressed as a linear combination of the
	  columns of <m>A</m> when <m>A</m> has a pivot position in every
	  row.  In this case, we said that the span of the vectors
	  <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m> is <m>\real^m</m>.  We
	  saw that at least <m>m</m> vectors are needed to span
	  <m>\real^m</m>. 
	</p></li>

	<li><title> Uniqueness </title>
	<p>
	  In this section, we studied the uniqueness of solutions to
	  the equation <m>A\xvec = \zerovec</m>, which is always
	  consistent.  When a nontrivial solution exists, we saw that
	  one vector of the set <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m>
	  is a linear combination of the others, in which case we said
	  that the set of vectors is linearly dependent.  This happens
	  when the matrix <m>A</m> has a column without a pivot
	  position.  We saw that there can be no more than <m>m</m>
	  vectors in a set of linearly independent vectors in
	  <m>\real^m</m>.  </p></li>
      </dl>
    </p>

    <p>
      To summarize the specific results of this section, we saw that:
    <ul>
      <li><p> A set of vectors is linearly dependent if one of the
      vectors is a linear combination of the others. </p></li>

      <li><p> A set of vectors is linearly independent if and only if
      the vectors form a matrix that has a pivot position in every
      column. </p></li>

      <li><p> A set of linearly independent vectors in <m>\real^m</m>
      contains no more than <m>m</m> vectors. </p></li>

      <li><p> The columns of the matrix <m>A</m> are linearly
      dependent if the homogeneous equation <m>A\xvec = \zerovec</m>
      has a nontrivial solution. </p></li>

      <li><p> A set of vectors <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m>
      is linearly dependent if there are weights
      <m>c_1,c_2,\ldots,c_n</m>, not all of which are zero, such that
      <me>
	c_1\vvec_1 + c_2\vvec_2 + \ldots + c_n\vvec_n = \zerovec
      </me>.
      </p></li>
    </ul></p>

  </subsection>

  <xi:include href="exercises/exercises2-4.xml" />
  
</section>

